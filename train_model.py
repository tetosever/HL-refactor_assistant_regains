#!/usr/bin/env python3
"""
Enhanced GCPN Training Script with Comprehensive Discriminator Monitoring
VERSIONE AVANZATA con controllo completo del discriminatore durante training adversariale

Key Features:
- Comprehensive discriminator performance monitoring
- Adaptive discriminator training frequency based on performance
- Early stopping and rollback mechanisms for discriminator degradation
- Performance trend analysis and automatic adjustments
- Detailed logging of all discriminator interactions
- Safeguards against discriminator collapse
"""
import datetime
import random
import time
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import numpy as np
import torch
import torch.nn.functional as F
import torch.optim as optim
import torch_geometric.data.data
from torch import nn, device
from torch.distributions import Categorical
from torch.utils.tensorboard import SummaryWriter
from torch_geometric.data import Batch, Data

# Import modules
from data_loader import create_unified_loader, validate_dataset_consistency, UnifiedDataLoader
from discriminator import HubDetectionDiscriminator
from hyperparameters_configuration import get_improved_config, print_config_summary
from policy_network import HubRefactoringPolicy
from rl_gym import HubRefactoringEnv
from training_logger import setup_optimized_logging, create_progress_tracker, AdvancedDiscriminatorMonitor
from global_graph_metrics import GlobalGraphMetrics, AdaptiveWeightScheduler



@dataclass
class RefactoringAction:
    """Refactoring action with pattern and parameters"""
    source_node: int
    target_node: int
    pattern: int
    terminate: bool
    confidence: float = 0.0

class EnhancedEnvironmentManager:
    """Manages diverse graph pools for better exploration"""

    def __init__(self, data_loader: UnifiedDataLoader, discriminator: nn.Module,
                 env_config, device: torch.device = torch.device('cpu')):
        self.data_loader = data_loader
        self.discriminator = discriminator
        self.pool_size = env_config.graph_pool_size
        self.device = device
        self.pool_refresh_episodes = env_config.pool_refresh_episodes

        # Graph pools
        self.smelly_pool = []
        self.pool_refresh_counter = 0
        self.total_graphs_seen = 0

        self._initialize_graph_pool()

    def _initialize_graph_pool(self):
        """Initialize diverse graph pool"""
        try:
            # Load larger sample for diversity
            smelly_data, labels = self.data_loader.load_dataset(
                max_samples_per_class=self.pool_size * 3,
                shuffle=True,
                validate_all=False,
                remove_metadata=True
            )

            # Filter smelly graphs only
            smelly_candidates = [data for data, label in zip(smelly_data, labels) if label == 1]

            if len(smelly_candidates) < self.pool_size:
                self.pool_size = len(smelly_candidates)

            # Select diverse subset
            self.smelly_pool = self._select_diverse_graphs(smelly_candidates, self.pool_size)

        except Exception as e:
            raise RuntimeError(f"Failed to initialize graph pool: {e}")

    def _select_diverse_graphs(self, candidates: List[Data], target_size: int) -> List[Data]:
        """Select diverse graphs based on size and complexity"""
        if len(candidates) <= target_size:
            return candidates

        env_config = get_improved_config()['environment']

        # Filter by size range
        size_filtered = [
            g for g in candidates
            if env_config.min_graph_size <= g.x.size(0) <= env_config.max_graph_size
        ]

        if not size_filtered:
            size_filtered = candidates

        # Sort by graph size for diversity
        size_sorted = sorted(size_filtered, key=lambda x: x.x.size(0))

        # Select spread across sizes using diversity bins
        selected = []
        if env_config.size_diversity_bins > 0:
            bin_size = len(size_sorted) // env_config.size_diversity_bins
            for i in range(env_config.size_diversity_bins):
                start_idx = i * bin_size
                end_idx = min((i + 1) * bin_size, len(size_sorted))
                if start_idx < len(size_sorted):
                    bin_graphs = size_sorted[start_idx:end_idx]
                    if bin_graphs:
                        selected.append(random.choice(bin_graphs))

        # Fill remaining with random selection
        remaining = [g for g in size_filtered if g not in selected]
        while len(selected) < target_size and remaining:
            selected.append(remaining.pop(random.randint(0, len(remaining) - 1)))

        return selected[:target_size]

    def get_random_graph(self) -> Data:
        """Get random graph from pool"""
        if not self.smelly_pool:
            self._refresh_pool()

        graph = random.choice(self.smelly_pool).clone()
        graph = graph.to(self.device)
        self.total_graphs_seen += 1

        return graph

    def _refresh_pool(self):
        """Refresh the graph pool with new samples"""
        self.pool_refresh_counter += 1
        try:
            self._initialize_graph_pool()
        except Exception as e:
            raise RuntimeError(f"Pool refresh failed: {e}")

    def should_refresh_pool(self, episode: int) -> bool:
        """Check if pool should be refreshed"""
        return episode > 0 and episode % self.pool_refresh_episodes == 0

    def get_stats(self) -> Dict[str, Any]:
        """Get pool statistics"""
        return {
            'pool_size': len(self.smelly_pool),
            'pool_refreshes': self.pool_refresh_counter,
            'total_graphs_seen': self.total_graphs_seen,
            'graph_sizes': [g.x.size(0) for g in self.smelly_pool] if self.smelly_pool else []
        }


class TensorBoardLogger:
    """Enhanced TensorBoard logging for RL training"""

    def __init__(self, log_dir: Path):
        self.log_dir = log_dir
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # Create timestamped run directory
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        run_dir = self.log_dir / f"rl_training_{timestamp}"

        self.writer = SummaryWriter(log_dir=str(run_dir))

    def log_episode_metrics(self, episode: int, metrics: Dict[str, float]):
        """Log episode-level metrics"""
        for key, value in metrics.items():
            self.writer.add_scalar(f"Episode/{key}", value, episode)

    def log_training_metrics(self, step: int, metrics: Dict[str, float]):
        """Log training-level metrics"""
        for key, value in metrics.items():
            self.writer.add_scalar(f"Training/{key}", value, step)

    def log_environment_metrics(self, episode: int, env_stats: Dict[str, Any]):
        """Log environment diversity metrics"""
        if 'pool_size' in env_stats:
            self.writer.add_scalar("Environment/pool_size", env_stats['pool_size'], episode)
        if 'total_graphs_seen' in env_stats:
            self.writer.add_scalar("Environment/total_graphs_seen", env_stats['total_graphs_seen'], episode)
        if 'graph_sizes' in env_stats and env_stats['graph_sizes']:
            sizes = env_stats['graph_sizes']
            self.writer.add_histogram("Environment/graph_sizes", np.array(sizes), episode)
            self.writer.add_scalar("Environment/avg_graph_size", np.mean(sizes), episode)

    def log_discriminator_metrics(self, episode: int, metrics: Dict[str, float]):
        """Log discriminator performance"""
        for key, value in metrics.items():
            self.writer.add_scalar(f"Discriminator/{key}", value, episode)

    def log_policy_metrics(self, step: int, metrics: Dict[str, float]):
        """Log policy learning metrics"""
        for key, value in metrics.items():
            self.writer.add_scalar(f"Policy/{key}", value, step)

    def close(self):
        """Close TensorBoard writer"""
        self.writer.close()


class PPOTrainer:
    """PPO trainer with centralized hyperparameter configuration and discriminator monitoring"""

    def __init__(self, policy: nn.Module, discriminator: nn.Module, discriminator_monitor: AdvancedDiscriminatorMonitor,
                 opt_config, disc_config, device: torch.device = torch.device('cpu')):
        self.device = device
        self.opt_config = opt_config
        self.disc_config = disc_config
        self.discriminator_monitor = discriminator_monitor

        # Move models to device
        self.policy = policy.to(device)
        self.discriminator = discriminator.to(device)

        # Create optimizers with adaptive parameters from monitor
        self.opt_policy = optim.AdamW(
            policy.parameters(),
            lr=opt_config.policy_lr,
            eps=opt_config.optimizer_eps,
            weight_decay=opt_config.weight_decay,
            betas=opt_config.betas
        )

        # Discriminator optimizer with adaptive learning rate
        adaptive_params = discriminator_monitor.get_adaptive_training_params()
        self.opt_disc = optim.AdamW(
            discriminator.parameters(),
            lr=adaptive_params['discriminator_lr'],
            eps=opt_config.optimizer_eps,
            weight_decay=opt_config.weight_decay,
            betas=opt_config.betas
        )

        # PPO parameters from config
        self.clip_eps = opt_config.clip_epsilon
        self.gamma = 0.99
        self.lam = 0.95
        self.value_coef = opt_config.value_coefficient
        self.entropy_coef = opt_config.entropy_coefficient
        self.max_grad_norm = opt_config.max_grad_norm

        # Learning rate schedulers from config
        self.policy_scheduler = optim.lr_scheduler.StepLR(
            self.opt_policy,
            step_size=opt_config.policy_lr_decay_step,
            gamma=opt_config.lr_decay_factor
        )
        self.disc_scheduler = optim.lr_scheduler.StepLR(
            self.opt_disc,
            step_size=opt_config.disc_lr_decay_step,
            gamma=opt_config.lr_decay_factor
        )

        # Training statistics
        self.update_count = 0
        self.policy_losses = []
        self.disc_losses = []
        self.disc_metrics = []
        self.policy_metrics = []

        # GPU optimization settings
        if device.type == 'cuda':
            torch.backends.cudnn.benchmark = True

    def update_discriminator_lr(self, new_lr: float):
        """Aggiorna learning rate del discriminatore dal monitor"""
        for param_group in self.opt_disc.param_groups:
            param_group['lr'] = new_lr

    def compute_gae(self, rewards: List[float], values: List[float],
                    dones: List[bool]) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute Generalized Advantage Estimation - GPU optimized"""
        if not rewards:
            return torch.tensor([], device=self.device), torch.tensor([], device=self.device)

        advantages = []
        returns = []

        gae = 0
        next_value = 0

        for t in reversed(range(len(rewards))):
            if dones[t]:
                next_value = 0
                gae = 0

            delta = rewards[t] + self.gamma * next_value - values[t]
            gae = delta + self.gamma * self.lam * gae

            advantages.insert(0, gae)
            returns.insert(0, gae + values[t])

            next_value = values[t]

        # Convert to tensors on device
        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)
        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)

        # Normalize advantages with numerical stability
        if len(advantages) > 1:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        return advantages, returns

    def update_discriminator(self, smelly_graphs: List[Data], clean_graphs: List[Data],
                             generated_graphs: List[Data], episode: int):
        """Discriminator update with enhanced monitoring and adaptive parameters"""

        # Get adaptive parameters from monitor
        adaptive_params = self.discriminator_monitor.get_adaptive_training_params()

        # Skip update if discriminator is critically degraded
        if adaptive_params['should_skip_update']:
            self.discriminator_monitor.logger.log_warning(
                f"âš ï¸  Skipping discriminator update due to critical degradation")
            return

        # Update learning rate if changed
        current_lr = self.opt_disc.param_groups[0]['lr']
        target_lr = adaptive_params['discriminator_lr']
        if abs(current_lr - target_lr) > 1e-7:
            self.update_discriminator_lr(target_lr)
            self.discriminator_monitor.logger.log_info(
                f"ðŸ”§ Discriminator LR updated: {current_lr:.2e} -> {target_lr:.2e}")

        epochs = self.opt_config.discriminator_epochs
        early_stop_threshold = self.disc_config.early_stop_threshold
        label_smoothing = self.disc_config.label_smoothing
        gp_lambda = self.disc_config.gradient_penalty_lambda

        total_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}
        epoch_losses = []

        self.discriminator_monitor.logger.log_info(
            f"ðŸ§  Starting discriminator update - Episode {episode} (LR: {target_lr:.2e})")

        for epoch in range(epochs):
            # Sample balanced batches with config-defined sizes
            min_size = min(len(smelly_graphs), len(clean_graphs), len(generated_graphs))
            batch_size = min(
                self.disc_config.max_samples_per_class,
                max(self.disc_config.min_samples_per_class, min_size)
            )

            if batch_size <= 2:
                self.discriminator_monitor.logger.log_warning(f"âš ï¸  Insufficient batch size: {batch_size}")
                continue

            try:
                # Sample data
                if self.disc_config.use_balanced_sampling:
                    smelly_sample = np.random.choice(len(smelly_graphs), batch_size, replace=False).tolist()
                    clean_sample = np.random.choice(len(clean_graphs), batch_size, replace=False).tolist()
                    generated_sample = np.random.choice(len(generated_graphs), batch_size, replace=False).tolist()
                else:
                    smelly_sample = random.sample(range(len(smelly_graphs)), batch_size)
                    clean_sample = random.sample(range(len(clean_graphs)), batch_size)
                    generated_sample = random.sample(range(len(generated_graphs)), batch_size)

                smelly_batch_data = [smelly_graphs[i].to(self.device) for i in smelly_sample]
                clean_batch_data = [clean_graphs[i].to(self.device) for i in clean_sample]
                generated_batch_data = [generated_graphs[i].to(self.device) for i in generated_sample]

                # Create batches
                smelly_batch = Batch.from_data_list(smelly_batch_data)
                clean_batch = Batch.from_data_list(clean_batch_data)
                generated_batch = Batch.from_data_list(generated_batch_data)

                # Enable gradients for gradient penalty
                smelly_batch.x.requires_grad_(True)
                clean_batch.x.requires_grad_(True)
                generated_batch.x.requires_grad_(True)

                # Forward pass
                self.discriminator.train()
                smelly_out = self.discriminator(smelly_batch)
                clean_out = self.discriminator(clean_batch)
                gen_out = self.discriminator(generated_batch)

                # Validate outputs
                if any(out is None or 'logits' not in out for out in [smelly_out, clean_out, gen_out]):
                    self.discriminator_monitor.logger.log_warning(f"âš ï¸  Invalid discriminator outputs at epoch {epoch}")
                    continue

                # Labels (smelly=1, clean/generated=0)
                smelly_labels = torch.ones(batch_size, dtype=torch.long, device=self.device)
                clean_labels = torch.zeros(batch_size, dtype=torch.long, device=self.device)
                gen_labels = torch.zeros(batch_size, dtype=torch.long, device=self.device)

                # Compute losses with label smoothing
                loss_smelly = F.cross_entropy(smelly_out['logits'], smelly_labels,
                                              label_smoothing=label_smoothing)
                loss_clean = F.cross_entropy(clean_out['logits'], clean_labels,
                                             label_smoothing=label_smoothing)
                loss_gen = F.cross_entropy(gen_out['logits'], gen_labels,
                                           label_smoothing=label_smoothing)

                # Gradient penalty calculation
                gp_loss = 0.0
                try:
                    for batch_data, out in [(smelly_batch, smelly_out),
                                            (clean_batch, clean_out),
                                            (generated_batch, gen_out)]:
                        if batch_data.x.grad is not None:
                            batch_data.x.grad.zero_()

                        grads = torch.autograd.grad(
                            outputs=out['logits'].sum(),
                            inputs=batch_data.x,
                            create_graph=True,
                            retain_graph=True,
                            only_inputs=True
                        )[0]
                        gp_loss += (grads.norm(2, dim=1) - 1).pow(2).mean()
                except Exception:
                    gp_loss = 0.0

                # Total discriminator loss
                disc_loss = loss_smelly + loss_clean + loss_gen + gp_lambda * gp_loss

                # Backward pass with gradient clipping
                self.opt_disc.zero_grad()
                disc_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), self.max_grad_norm)
                self.opt_disc.step()

                epoch_losses.append(disc_loss.item())

                # Calculate detailed metrics if enabled
                if self.disc_config.calculate_detailed_metrics:
                    with torch.no_grad():
                        self.discriminator.eval()

                        # Get predictions
                        smelly_preds = smelly_out['logits'].argmax(dim=1)
                        clean_preds = clean_out['logits'].argmax(dim=1)
                        gen_preds = gen_out['logits'].argmax(dim=1)

                        # Combine all predictions and labels
                        all_preds = torch.cat([smelly_preds, clean_preds, gen_preds])
                        all_labels = torch.cat([smelly_labels, clean_labels, gen_labels])

                        # Calculate metrics
                        correct = (all_preds == all_labels).float()
                        accuracy = correct.mean().item()

                        # Calculate precision, recall, F1 for positive class (smelly)
                        tp = ((all_preds == 1) & (all_labels == 1)).sum().float()
                        fp = ((all_preds == 1) & (all_labels == 0)).sum().float()
                        fn = ((all_preds == 0) & (all_labels == 1)).sum().float()

                        precision = tp / (tp + fp + 1e-8)
                        recall = tp / (tp + fn + 1e-8)
                        f1 = 2 * precision * recall / (precision + recall + 1e-8)

                        # Store metrics
                        total_metrics['accuracy'].append(accuracy)
                        total_metrics['precision'].append(precision.item())
                        total_metrics['recall'].append(recall.item())
                        total_metrics['f1'].append(f1.item())

                self.disc_losses.append(disc_loss.item())

                # Early stopping based on performance
                if total_metrics['accuracy'] and total_metrics['f1']:
                    current_acc = total_metrics['accuracy'][-1]
                    current_f1 = total_metrics['f1'][-1]

                    # Early stopping if performance is very poor
                    if current_acc < early_stop_threshold and current_f1 < early_stop_threshold:
                        self.discriminator_monitor.logger.log_warning(
                            f"âš ï¸  Early stopping at epoch {epoch} due to poor performance")
                        break

                    # Early stopping if performance degradation is severe
                    if (current_acc < 0.4 and
                            self.discriminator_monitor.perf_state.baseline_accuracy > 0.6):
                        self.discriminator_monitor.logger.log_warning(f"ðŸš¨ Early stopping due to severe degradation")
                        break

            except Exception as e:
                self.discriminator_monitor.logger.log_warning(f"âš ï¸  Discriminator training error at epoch {epoch}: {e}")
                continue

        # Calculate and log average metrics across epochs
        if total_metrics['accuracy']:
            avg_metrics = {
                'accuracy': np.mean(total_metrics['accuracy']),
                'precision': np.mean(total_metrics['precision']),
                'recall': np.mean(total_metrics['recall']),
                'f1': np.mean(total_metrics['f1']),
                'loss': np.mean(epoch_losses) if epoch_losses else 0.0
            }

            self.disc_metrics.append(avg_metrics)

            # Log training results
            self.discriminator_monitor.logger.log_info(
                f"ðŸ§  Discriminator training completed - "
                f"Acc: {avg_metrics['accuracy']:.3f}, F1: {avg_metrics['f1']:.3f}, "
                f"Loss: {avg_metrics['loss']:.4f}"
            )
        else:
            self.discriminator_monitor.logger.log_warning(
                "âš ï¸  No valid metrics calculated during discriminator training")

        # Step learning rate scheduler
        self.disc_scheduler.step()

        # GPU memory cleanup
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()

    def update_policy(self, states: List[Data], actions: List[Any],
                      old_log_probs: torch.Tensor, advantages: torch.Tensor,
                      returns: torch.Tensor):
        """Policy update with centralized configuration"""

        epochs = self.opt_config.policy_epochs

        if len(states) == 0:
            return

        policy_losses = []
        entropy_losses = []
        kl_divergences = []

        for epoch in range(epochs):
            # Shuffle data for better training
            indices = torch.randperm(len(states), device=self.device)
            batch_size = min(24, len(states))

            for start in range(0, len(states), batch_size):
                end = min(start + batch_size, len(states))
                batch_indices = indices[start:end].cpu().numpy()

                # Create batch
                batch_states = [states[i] for i in batch_indices]
                batch_actions = [actions[i] for i in batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_advantages = advantages[batch_indices]

                # Compute losses
                loss_info = self.compute_policy_loss(batch_states, batch_actions,
                                                     batch_old_log_probs, batch_advantages)

                # Total loss with config weights
                total_loss = (loss_info['policy_loss'] +
                              self.value_coef * 0 +
                              -self.entropy_coef * loss_info['entropy'])

                # Backward pass with gradient clipping
                self.opt_policy.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                self.opt_policy.step()

                # Store metrics
                policy_losses.append(loss_info['policy_loss'].item())
                entropy_losses.append(loss_info['entropy'].item())
                kl_divergences.append(loss_info['approx_kl'].item())

                # Early stopping on large KL divergence
                if loss_info['approx_kl'] > 0.02:
                    break

        # Store average metrics
        if policy_losses:
            avg_policy_loss = np.mean(policy_losses)
            avg_entropy = np.mean(entropy_losses)
            avg_kl = np.mean(kl_divergences)

            self.policy_losses.append(avg_policy_loss)
            self.policy_metrics.append({
                'policy_loss': avg_policy_loss,
                'entropy': avg_entropy,
                'kl_divergence': avg_kl,
                'update_count': self.update_count
            })

        self.update_count += 1

        # Step learning rate scheduler
        self.policy_scheduler.step()

        # GPU memory cleanup
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()

    def compute_policy_loss(self, states: List[Data], actions: List[Any],
                            old_log_probs: torch.Tensor, advantages: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Policy loss computation with better error handling"""

        # Validate inputs
        if not states or not actions:
            return {
                'policy_loss': torch.tensor(0.0, device=self.device, requires_grad=True),
                'entropy': torch.tensor(0.0, device=self.device),
                'approx_kl': torch.tensor(0.0, device=self.device),
                'clipfrac': torch.tensor(0.0, device=self.device)
            }

        # Ensure all states have correct format
        valid_states = []
        valid_actions = []
        valid_indices = []

        for i, state in enumerate(states):
            if state.x.device != self.device:
                state = state.to(self.device)

            # Validate 7-feature format
            if state.x.size(1) != 7:
                continue

            # Ensure batch attribute
            if not hasattr(state, 'batch') or state.batch is None:
                state.batch = torch.zeros(state.x.size(0), dtype=torch.long, device=self.device)

            valid_states.append(state)
            valid_actions.append(actions[i])
            valid_indices.append(i)

        if not valid_states:
            return {
                'policy_loss': torch.tensor(0.0, device=self.device, requires_grad=True),
                'entropy': torch.tensor(0.0, device=self.device),
                'approx_kl': torch.tensor(0.0, device=self.device),
                'clipfrac': torch.tensor(0.0, device=self.device)
            }

        # Filter old_log_probs and advantages to match valid states
        if len(valid_indices) < len(old_log_probs):
            old_log_probs = old_log_probs[valid_indices]
            advantages = advantages[valid_indices]

        # Batch states with better error handling
        try:
            batch_data = Batch.from_data_list(valid_states)
            if batch_data.x.device != self.device:
                batch_data = batch_data.to(self.device)
        except Exception:
            return {
                'policy_loss': torch.tensor(0.0, device=self.device, requires_grad=True),
                'entropy': torch.tensor(0.0, device=self.device),
                'approx_kl': torch.tensor(0.0, device=self.device),
                'clipfrac': torch.tensor(0.0, device=self.device)
            }

        # Forward pass
        try:
            self.policy.train()
            policy_out = self.policy(batch_data)
        except Exception:
            return {
                'policy_loss': torch.tensor(0.0, device=self.device, requires_grad=True),
                'entropy': torch.tensor(0.0, device=self.device),
                'approx_kl': torch.tensor(0.0, device=self.device),
                'clipfrac': torch.tensor(0.0, device=self.device)
            }

        # Compute current log probabilities with improved error handling
        log_probs = []
        entropies = []

        # Handle batched data properly
        num_graphs = len(valid_states)
        nodes_per_graph = [state.x.size(0) for state in valid_states]
        cumsum_nodes = [0] + list(np.cumsum(nodes_per_graph))

        for i, action in enumerate(valid_actions):
            try:
                start_idx = cumsum_nodes[i]
                end_idx = cumsum_nodes[i + 1]

                # Validate indices
                if start_idx >= policy_out['hub_probs'].size(0) or end_idx > policy_out['hub_probs'].size(0):
                    log_probs.append(torch.tensor(-5.0, device=self.device))
                    entropies.append(torch.tensor(0.1, device=self.device))
                    continue

                # Hub selection with better bounds checking
                hub_probs_i = policy_out['hub_probs'][start_idx:end_idx]
                if hub_probs_i.size(0) == 0:
                    log_probs.append(torch.tensor(-5.0, device=self.device))
                    entropies.append(torch.tensor(0.1, device=self.device))
                    continue

                # Add numerical stability
                hub_probs_i = hub_probs_i + 1e-8
                hub_probs_i = hub_probs_i / hub_probs_i.sum()

                hub_dist = Categorical(hub_probs_i)
                hub_action_tensor = torch.tensor(
                    min(action.source_node, hub_probs_i.size(0) - 1),
                    device=self.device
                )

                log_prob_hub = hub_dist.log_prob(hub_action_tensor)
                entropy_hub = hub_dist.entropy()

                # Pattern selection with better error handling
                pattern_idx = start_idx + min(action.source_node, hub_probs_i.size(0) - 1)
                if pattern_idx < policy_out['pattern_probs'].size(0):
                    pattern_probs_i = policy_out['pattern_probs'][pattern_idx] + 1e-8
                    pattern_probs_i = pattern_probs_i / pattern_probs_i.sum()

                    pattern_dist = Categorical(pattern_probs_i)
                    pattern_action_tensor = torch.tensor(
                        min(action.pattern, pattern_probs_i.size(0) - 1),
                        device=self.device
                    )

                    log_prob_pattern = pattern_dist.log_prob(pattern_action_tensor)
                    entropy_pattern = pattern_dist.entropy()
                else:
                    log_prob_pattern = torch.tensor(-2.0, device=self.device)
                    entropy_pattern = torch.tensor(0.1, device=self.device)

                # Termination with better handling
                if i < policy_out['term_probs'].size(0):
                    term_probs_i = policy_out['term_probs'][i] + 1e-8
                    term_probs_i = term_probs_i / term_probs_i.sum()
                else:
                    term_probs_i = torch.tensor([0.7, 0.3], device=self.device)

                term_dist = Categorical(term_probs_i)
                term_action_tensor = torch.tensor(int(action.terminate), device=self.device)
                log_prob_term = term_dist.log_prob(term_action_tensor)
                entropy_term = term_dist.entropy()

                # Combine probabilities
                total_log_prob = log_prob_hub + log_prob_pattern + log_prob_term
                total_entropy = entropy_hub + entropy_pattern + entropy_term

                log_probs.append(total_log_prob)
                entropies.append(total_entropy)

            except Exception:
                log_probs.append(torch.tensor(-5.0, device=self.device))
                entropies.append(torch.tensor(0.1, device=self.device))

        if not log_probs:
            return {
                'policy_loss': torch.tensor(0.0, device=self.device, requires_grad=True),
                'entropy': torch.tensor(0.0, device=self.device),
                'approx_kl': torch.tensor(0.0, device=self.device),
                'clipfrac': torch.tensor(0.0, device=self.device)
            }

        log_probs = torch.stack(log_probs)
        entropy = torch.stack(entropies).mean()

        # Ensure dimensions match
        old_log_probs = old_log_probs.to(self.device)
        if old_log_probs.size(0) != log_probs.size(0):
            min_size = min(old_log_probs.size(0), log_probs.size(0))
            old_log_probs = old_log_probs[:min_size]
            log_probs = log_probs[:min_size]
            advantages = advantages[:min_size]

        # PPO clipped objective with numerical stability
        ratio = torch.exp(torch.clamp(log_probs - old_log_probs, -10, 10))
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()

        # Compute additional metrics
        with torch.no_grad():
            approx_kl = (old_log_probs - log_probs).mean()
            clipfrac = ((ratio - 1).abs() > self.clip_eps).float().mean()

        return {
            'policy_loss': policy_loss,
            'entropy': entropy,
            'approx_kl': approx_kl,
            'clipfrac': clipfrac
        }

    def get_latest_metrics(self) -> Dict[str, Any]:
        """Get latest training metrics for logging"""
        metrics = {}

        if self.policy_metrics:
            latest_policy = self.policy_metrics[-1]
            metrics.update({
                'policy_loss': latest_policy['policy_loss'],
                'policy_entropy': latest_policy['entropy'],
                'policy_kl': latest_policy['kl_divergence']
            })

        if self.disc_metrics:
            latest_disc = self.disc_metrics[-1]
            metrics.update({
                'discriminator_accuracy': latest_disc['accuracy'],
                'discriminator_f1': latest_disc['f1'],
                'discriminator_precision': latest_disc['precision'],
                'discriminator_recall': latest_disc['recall'],
                'discriminator_loss': latest_disc.get('loss', 0.0)
            })

        return metrics


def safe_torch_load(file_path: Path, device: torch.device):
    """Safely load PyTorch files with PyTorch Geometric compatibility"""
    try:
        with torch.serialization.safe_globals([torch_geometric.data.data.DataEdgeAttr]):
            return torch.load(file_path, map_location=device, weights_only=True)
    except Exception:
        return torch.load(file_path, map_location=device, weights_only=False)


def load_pretrained_discriminator(model_path: Path, device: torch.device, logger) -> Tuple[nn.Module, Dict]:
    """Load pretrained discriminator with unified format validation"""
    logger.log_info(f"Loading pretrained discriminator from {model_path}")

    if not model_path.exists():
        raise FileNotFoundError(f"Pretrained discriminator not found at {model_path}")

    checkpoint = safe_torch_load(model_path, device)

    # Validate unified data pipeline information
    if 'unified_data_pipeline' in checkpoint:
        pipeline_info = checkpoint['unified_data_pipeline']
        if not pipeline_info.get('consistent_with_rl_training', False):
            logger.log_warning("Discriminator may not be fully compatible with RL training")
    else:
        logger.log_warning("Model lacks unified data pipeline information - compatibility uncertain")

    # Get architecture parameters from checkpoint
    model_architecture = checkpoint.get('model_architecture', {})
    node_dim = checkpoint.get('node_dim', 7)
    edge_dim = checkpoint.get('edge_dim', 1)

    # Validate unified format expectations
    if node_dim != 7:
        raise ValueError(f"Discriminator has incompatible feature dimensions: {node_dim} != 7")

    # Use saved architecture parameters or defaults
    hidden_dim = model_architecture.get('hidden_dim', 128)
    num_layers = model_architecture.get('num_layers', 4)
    dropout = model_architecture.get('dropout', 0.15)
    heads = model_architecture.get('heads', 8)

    # Create model with correct architecture
    discriminator = HubDetectionDiscriminator(
        node_dim=node_dim,
        edge_dim=edge_dim,
        hidden_dim=hidden_dim,
        num_layers=num_layers,
        dropout=dropout,
        heads=heads
    ).to(device)

    # Load state dict
    discriminator.load_state_dict(checkpoint['model_state_dict'])
    discriminator.eval()

    # Log performance info
    cv_results = checkpoint.get('cv_results', {})
    if cv_results:
        mean_f1 = cv_results.get('mean_f1', 0)
        mean_acc = cv_results.get('mean_accuracy', 0)
        logger.log_info(f"Loaded discriminator with CV F1: {mean_f1:.4f}, Accuracy: {mean_acc:.4f}")

    return discriminator, checkpoint


def create_diverse_training_environments(env_manager: EnhancedEnvironmentManager,
                                         discriminator: nn.Module, training_config,
                                         device: torch.device = torch.device('cpu')) -> List[HubRefactoringEnv]:
    """Create training environments with diverse graphs using centralized config"""
    num_envs = training_config.num_envs
    max_steps = training_config.max_steps_per_episode

    envs = []
    for i in range(num_envs):
        try:
            # Get random graph from diverse pool
            initial_graph = env_manager.get_random_graph()

            # Create environment with config-based max_steps
            env = HubRefactoringEnv(initial_graph, discriminator, max_steps=max_steps, device=device)
            env.set_current_episode(0)
            # Store reference to environment manager for dynamic resets
            env.env_manager = env_manager
            envs.append(env)

        except Exception:
            continue

    return envs


def enhanced_reset_environment(env: HubRefactoringEnv) -> Data:
    """Enhanced reset that picks new graph from pool"""
    try:
        if hasattr(env, 'env_manager'):
            # Get new diverse graph
            new_graph = env.env_manager.get_random_graph()

            # Update environment with new graph
            env.initial_data = new_graph.to(env.device)
            env.current_data = None
            env.steps = 0
            env.action_history.clear()
            env._cached_hub_score = None
            env._graph_hash = None

            # Reset with new graph
            return env.reset()
        else:
            # Fallback to original reset
            return env.reset()

    except Exception:
        return env.reset()


def collect_rollouts(envs: List[HubRefactoringEnv], policy: nn.Module,
                     training_config, device: torch.device = torch.device('cpu')) -> Dict[str, List]:
    """Collect rollouts with proper metric transport"""

    steps_per_env = training_config.steps_per_env

    all_states = []
    all_actions = []
    all_rewards = []
    all_dones = []
    all_log_probs = []
    all_values = []

    # Add containers for step-level metrics
    all_step_infos = []
    all_hub_improvements = []
    all_hub_scores = []
    episode_step_metrics = {
        'total_steps': 0,
        'successful_steps': 0,
        'valid_metrics_steps': 0,
        'total_hub_improvement': 0.0,
        'hub_score_changes': [],
        'step_errors': [],
        'fallback_scores_count': 0
    }

    # Reset environments
    states = []
    valid_envs = []

    for i, env in enumerate(envs):
        try:
            state = env.reset()
            if state.x.device != device:
                state = state.to(device)

            # All states should already have 7 features from unified pipeline
            if state.x.size(1) != 7:
                continue

            states.append(state)
            valid_envs.append(env)
        except Exception:
            continue

    if not states:
        return {
            'states': [], 'actions': [], 'rewards': [], 'dones': [],
            'log_probs': torch.tensor([], device=device), 'values': [],
            'step_infos': [],
            'episode_metrics': episode_step_metrics
        }

    enhanced_hub_metrics = []

    for step in range(steps_per_env):
        # Ensure all states are on correct device
        states_on_device = []
        for state in states:
            if state.x.device != device:
                state = state.to(device)
            if not hasattr(state, 'batch'):
                state.batch = torch.zeros(state.x.size(0), dtype=torch.long, device=device)
            states_on_device.append(state)

        # Batch states
        try:
            batch_data = Batch.from_data_list(states_on_device)
            if batch_data.x.device != device:
                batch_data = batch_data.to(device)
        except Exception:
            break

        # Policy forward pass
        with torch.no_grad():
            try:
                policy_out = policy(batch_data)
                values = []
                for state in states_on_device:
                    # Use discriminator for value estimation
                    pred = None
                    for env in valid_envs:
                        try:
                            pred = env.discriminator.eval()
                            with torch.no_grad():
                                output = env.discriminator(state)
                                if output and 'logits' in output:
                                    probs = F.softmax(output['logits'], dim=1)
                                    pred = probs[0, 1].item()
                                    break
                        except:
                            continue

                    value = 1.0 - pred if pred is not None else 0.5
                    values.append(value)
                values = torch.tensor(values, device=device)
            except Exception:
                break

        # Sample actions
        actions = []
        log_probs = []
        num_graphs = len(states_on_device)
        nodes_per_graph = [state.x.size(0) for state in states_on_device]
        cumsum_nodes = [0] + list(np.cumsum(nodes_per_graph))

        for i in range(num_graphs):
            try:
                start_idx = cumsum_nodes[i]
                end_idx = cumsum_nodes[i + 1]
                num_nodes = nodes_per_graph[i]

                if start_idx >= policy_out['hub_probs'].size(0) or end_idx > policy_out['hub_probs'].size(0):
                    action = RefactoringAction(0, min(1, num_nodes - 1), 0, False)
                    actions.append(action)
                    log_probs.append(torch.tensor(0.0, device=device))
                    continue

                # Sample hub
                hub_probs_i = policy_out['hub_probs'][start_idx:end_idx]
                if hub_probs_i.size(0) == 0:
                    action = RefactoringAction(0, min(1, num_nodes - 1), 0, False)
                    actions.append(action)
                    log_probs.append(torch.tensor(0.0, device=device))
                    continue

                hub_probs_i = hub_probs_i + 1e-8
                hub_dist = Categorical(hub_probs_i)
                hub_local = hub_dist.sample().item()
                hub_local = min(hub_local, num_nodes - 1)

                # Sample pattern
                pattern_idx = start_idx + hub_local
                if pattern_idx < policy_out['pattern_probs'].size(0):
                    pattern_probs_i = policy_out['pattern_probs'][pattern_idx] + 1e-8
                    pattern_dist = Categorical(pattern_probs_i)
                    pattern = pattern_dist.sample().item()
                else:
                    pattern = 0

                # Sample termination
                if i < policy_out['term_probs'].size(0):
                    term_probs_i = policy_out['term_probs'][i] + 1e-8
                else:
                    term_probs_i = torch.tensor([0.7, 0.3], device=device)

                term_dist = Categorical(term_probs_i)
                terminate = term_dist.sample().item() == 1

                # Target selection
                if num_nodes > 1:
                    target_candidates = [j for j in range(num_nodes) if j != hub_local]
                    target_local = random.choice(target_candidates) if target_candidates else hub_local
                else:
                    target_local = hub_local

                action = RefactoringAction(hub_local, target_local, pattern, terminate)
                actions.append(action)

                # Compute log probability
                log_prob = (hub_dist.log_prob(torch.tensor(hub_local, device=device)) +
                            pattern_dist.log_prob(torch.tensor(pattern, device=device)) +
                            term_dist.log_prob(torch.tensor(int(terminate), device=device)))
                log_probs.append(log_prob)

            except Exception:
                action = RefactoringAction(0, 0, 0, False)
                actions.append(action)
                log_probs.append(torch.tensor(0.0, device=device))

        # Execute actions and collect all step metrics
        next_states = []
        rewards = []
        dones = []
        step_infos_batch = []

        for i, (env, action) in enumerate(zip(valid_envs, actions)):
            try:
                state, reward, done, info = env.step(
                    (action.source_node, action.target_node, action.pattern, action.terminate)
                )

                if state.x.device != device:
                    state = state.to(device)
                if not hasattr(state, 'batch'):
                    state.batch = torch.zeros(state.x.size(0), dtype=torch.long, device=device)

                next_states.append(state)
                rewards.append(reward)
                dones.append(done)

                # Store step info for metric transport
                step_infos_batch.append(info)

                # Update episode metrics from step info
                episode_step_metrics['total_steps'] += 1
                if info.get('step_metrics_calculated', False):
                    episode_step_metrics['valid_metrics_steps'] += 1

                if info.get('success', False):
                    episode_step_metrics['successful_steps'] += 1

                if 'hub_improvement' in info:
                    hub_improvement = info['hub_improvement']
                    episode_step_metrics['total_hub_improvement'] += hub_improvement
                    all_hub_improvements.append(hub_improvement)

                if 'new_hub_score' in info:
                    all_hub_scores.append(info['new_hub_score'])
                    episode_step_metrics['hub_score_changes'].append({
                        'env_id': i,
                        'step': step,
                        'old_score': info.get('old_hub_score', 0.5),
                        'new_score': info['new_hub_score'],
                        'improvement': info.get('hub_improvement', 0.0)
                    })

                if info.get('hub_score_is_fallback', False):
                    episode_step_metrics['fallback_scores_count'] += 1

                if 'step_error' in info and info['step_error']:
                    episode_step_metrics['step_errors'].append({
                        'env_id': i,
                        'step': step,
                        'error': info['step_error']
                    })

                # Store experience with step info
                all_states.append(states_on_device[i])
                all_actions.append(action)
                all_rewards.append(reward)
                all_dones.append(done)
                all_log_probs.append(log_probs[i] if i < len(log_probs) else torch.tensor(0.0, device=device))
                all_values.append(values[i].item() if i < len(values) else 0.0)
                all_step_infos.append(info)

                # Reset if done
                if done:
                    try:
                        reset_state = enhanced_reset_environment(env)
                        if reset_state.x.device != device:
                            reset_state = reset_state.to(device)
                        if not hasattr(reset_state, 'batch'):
                            reset_state.batch = torch.zeros(reset_state.x.size(0), dtype=torch.long, device=device)
                        next_states[i] = reset_state
                    except Exception:
                        pass

                if step % 5 == 0:  # Every 5th step to avoid overhead
                    try:
                        breakdown = env.get_hub_score_breakdown()
                        enhanced_hub_metrics.append({
                            'env_id': i,
                            'step': step,
                            'episode': env.current_episode,
                            'breakdown': breakdown
                        })
                    except Exception:
                        pass

            except Exception as e:
                # Create fallback info even for errors
                error_info = {
                    'valid': False,
                    'success': False,
                    'hub_improvement': 0.0,
                    'old_hub_score': 0.5,
                    'new_hub_score': 0.5,
                    'step_error': str(e),
                    'step_metrics_calculated': False
                }
                step_infos_batch.append(error_info)
                episode_step_metrics['step_errors'].append({
                    'env_id': i,
                    'step': step,
                    'error': str(e)
                })

                if i < len(states_on_device):
                    next_states.append(states_on_device[i])
                else:
                    fallback_state = states_on_device[0].clone() if states_on_device else None
                    if fallback_state is not None:
                        next_states.append(fallback_state)
                    else:
                        break

                rewards.append(-0.5)
                dones.append(False)

                # Store error experience
                if i < len(states_on_device):
                    all_states.append(states_on_device[i])
                    all_actions.append(action)
                    all_rewards.append(-0.5)
                    all_dones.append(False)
                    all_log_probs.append(log_probs[i] if i < len(log_probs) else torch.tensor(0.0, device=device))
                    all_values.append(0.0)
                    all_step_infos.append(error_info)

        states = next_states

        # Break if all environments are done or we have no valid states
        if not states or all(dones):
            break

        # GPU memory management
        if device.type == 'cuda' and step % 10 == 0:
            torch.cuda.empty_cache()

    if not all_log_probs:
        return {
            'states': [], 'actions': [], 'rewards': [], 'dones': [],
            'log_probs': torch.tensor([], device=device), 'values': [],
            'step_infos': [],
            'episode_metrics': episode_step_metrics
        }

    # Return with step-level metrics
    return {
        'states': all_states,
        'actions': all_actions,
        'rewards': all_rewards,
        'dones': all_dones,
        'log_probs': torch.stack(all_log_probs) if all_log_probs else torch.tensor([], device=device),
        'values': all_values,
        'step_infos': all_step_infos,
        'hub_improvements': all_hub_improvements,
        'hub_scores': all_hub_scores,
        'episode_metrics': episode_step_metrics,
        'enhanced_hub_metrics': enhanced_hub_metrics  # NEW
    }


def analyze_enhanced_hub_score_trends(envs: List[HubRefactoringEnv], episode: int, logger) -> Dict[str, Any]:
    """
    Analyze trends in enhanced hub score components across environments
    """
    try:
        all_breakdowns = []
        phase_distribution = {}

        for env in envs:
            try:
                breakdown = env.get_hub_score_breakdown()
                if breakdown and 'component_scores' in breakdown:
                    all_breakdowns.append(breakdown)

                    phase = breakdown.get('training_phase', 'unknown')
                    phase_distribution[phase] = phase_distribution.get(phase, 0) + 1

            except Exception:
                continue

        if not all_breakdowns:
            return {'status': 'no_data'}

        # Calculate averages across environments
        avg_components = {}
        component_keys = ['gini', 'degree_centralization', 'betweenness_centralization', 'discriminator', 'composite']

        for key in component_keys:
            values = []
            for breakdown in all_breakdowns:
                components = breakdown.get('component_scores', {})
                if key in components:
                    values.append(components[key])

            if values:
                avg_components[f'avg_{key}'] = sum(values) / len(values)
                avg_components[f'std_{key}'] = np.std(values) if len(values) > 1 else 0.0

        # Get current weights (should be same across all environments)
        current_weights = all_breakdowns[0].get('current_weights', {})

        analysis = {
            'status': 'success',
            'episode': episode,
            'num_environments': len(all_breakdowns),
            'phase_distribution': phase_distribution,
            'average_components': avg_components,
            'current_weights': current_weights,
            'dominant_phase': max(phase_distribution.items(), key=lambda x: x[1])[
                0] if phase_distribution else 'unknown'
        }

        return analysis

    except Exception as e:
        logger.log_warning(f"Enhanced hub score trend analysis failed: {e}")
        return {'status': 'error', 'error': str(e)}

def setup_gpu_environment(logger):
    """Setup optimal GPU environment"""
    if torch.cuda.is_available():
        # Set memory management
        torch.cuda.empty_cache()

        # Get GPU info
        gpu_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1e9

        logger.log_info(f"GPU Setup: {gpu_count} GPUs, Current: {gpu_name}, Memory: {gpu_memory:.1f} GB")

        # Optimize for memory
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

        device = torch.device('cuda')
    else:
        logger.log_warning("CUDA not available, using CPU")
        device = torch.device('cpu')

    return device


def monitor_gpu_usage(logger):
    """Monitor GPU memory usage"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        max_memory = torch.cuda.max_memory_allocated() / 1e9

        if allocated > 8.0:  # If using more than 8GB
            torch.cuda.empty_cache()
            logger.log_info("GPU memory cleanup performed")


def main():
    """Main training loop with comprehensive discriminator monitoring"""
    # Setup optimized logging
    logger = setup_optimized_logging()

    # Load centralized configuration
    config = get_improved_config()
    reward_config = config['rewards']
    training_config = config['training']
    opt_config = config['optimization']
    disc_config = config['discriminator']
    env_config = config['environment']

    # Start training with config
    logger.start_training(config)

    # Print configuration summary (only once)
    print_config_summary()

    # Setup GPU environment
    device = setup_gpu_environment(logger)

    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if device.type == 'cuda':
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)

    # Paths
    base_dir = Path(__file__).parent
    data_dir = base_dir / 'dataset_builder' / 'data' / 'dataset_graph_feature'
    results_dir = base_dir / 'results'
    discriminator_path = results_dir / 'discriminator_pretraining' / 'pretrained_discriminator.pt'

    results_dir.mkdir(exist_ok=True)

    # TensorBoard setup
    tensorboard_dir = results_dir / 'tensorboard_logs'
    tb_logger = TensorBoardLogger(tensorboard_dir)

    # Check if pretrained discriminator exists
    if not discriminator_path.exists():
        logger.log_error(f"Pretrained discriminator not found at {discriminator_path}")
        return

    # Create unified data loader
    try:
        data_loader = create_unified_loader(data_dir, device)
        stats = data_loader.get_dataset_statistics()
        consistency_report = validate_dataset_consistency(data_dir, sample_size=100)

    except Exception as e:
        logger.log_error(f"Failed to create unified data loader: {e}")
        return

    # Load pretrained discriminator
    try:
        discriminator, discriminator_checkpoint = load_pretrained_discriminator(discriminator_path, device, logger)

        # Initialize comprehensive discriminator monitor
        discriminator_monitor = AdvancedDiscriminatorMonitor(
            discriminator=discriminator,
            data_loader=data_loader,
            device=device,
            logger=logger
        )

        # Set baseline from checkpoint
        baseline_metrics = {
            'accuracy': discriminator_checkpoint.get('cv_results', {}).get('mean_accuracy', 0.0),
            'f1': discriminator_checkpoint.get('cv_results', {}).get('mean_f1', 0.0),
            'precision': discriminator_checkpoint.get('cv_results', {}).get('mean_precision', 0.0),
            'recall': discriminator_checkpoint.get('cv_results', {}).get('mean_recall', 0.0)
        }
        discriminator_monitor.set_baseline_performance(baseline_metrics)

        # Initial validation
        logger.log_info("ðŸ” Performing initial discriminator validation...")
        initial_validation = discriminator_monitor.validate_performance(episode=0, detailed=True)

        if initial_validation['accuracy'] < 0.1:
            logger.log_error("Discriminator performance is critically low!")
            return
        elif initial_validation['failed_predictions'] > initial_validation['total_samples'] // 2:
            logger.log_warning(f"High failure rate: {initial_validation['failed_predictions']} failed predictions")

    except Exception as e:
        logger.log_error(f"Failed to load pretrained discriminator: {e}")
        return

    # Model parameters - FIXED for unified 7-feature format
    hidden_dim = discriminator_checkpoint.get('model_architecture', {}).get('hidden_dim', 128)

    # Create policy network
    policy = HubRefactoringPolicy(
        node_dim=7,
        edge_dim=1,
        hidden_dim=hidden_dim,
        hub_bias_strength=4.0,
        exploration_rate=0.1
    ).to(device)

    total_params = sum(p.numel() for p in policy.parameters())
    trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)
    logger.log_info(f"Policy network - Total params: {total_params:,}, Trainable: {trainable_params:,}")

    # Setup PPO trainer with discriminator monitor
    trainer = PPOTrainer(policy, discriminator, discriminator_monitor, opt_config, disc_config, device=device)

    # Training parameters from centralized config
    num_episodes = training_config.num_episodes
    num_envs = min(training_config.num_envs if device.type == 'cuda' else 3, stats['label_distribution']['smelly'])
    steps_per_env = training_config.steps_per_env
    update_frequency = training_config.update_frequency
    environment_refresh_frequency = training_config.environment_refresh_frequency

    # Hub score tracking parameters
    hub_score_improvement_threshold = training_config.hub_improvement_threshold
    consecutive_failures_threshold = training_config.consecutive_failures_threshold

    # Create enhanced environment manager
    try:
        env_manager = EnhancedEnvironmentManager(
            data_loader=data_loader,
            discriminator=discriminator,
            env_config=env_config,
            device=device
        )
    except Exception as e:
        logger.log_error(f"Failed to create environment manager: {e}")
        return

    # Create environments
    try:
        envs = create_diverse_training_environments(env_manager, discriminator, training_config, device)
    except Exception as e:
        logger.log_error(f"Failed to create training environments: {e}")
        return

    # Update environment reward structure
    for i, env in enumerate(envs):
        try:
            env.REWARD_SUCCESS = reward_config.REWARD_SUCCESS
            env.REWARD_PARTIAL_SUCCESS = reward_config.REWARD_PARTIAL_SUCCESS
            env.REWARD_FAILURE = reward_config.REWARD_FAILURE
            env.REWARD_STEP = reward_config.REWARD_STEP
            env.REWARD_HUB_REDUCTION = reward_config.REWARD_HUB_REDUCTION
            env.REWARD_INVALID = reward_config.REWARD_INVALID
            env.REWARD_SMALL_IMPROVEMENT = reward_config.REWARD_SMALL_IMPROVEMENT

            env.HUB_SCORE_EXCELLENT = reward_config.HUB_SCORE_EXCELLENT
            env.HUB_SCORE_GOOD = reward_config.HUB_SCORE_GOOD
            env.HUB_SCORE_ACCEPTABLE = reward_config.HUB_SCORE_ACCEPTABLE

            env.improvement_multiplier_boost = reward_config.improvement_multiplier_boost
            env.improvement_multiplier_decay = reward_config.improvement_multiplier_decay
            env.improvement_multiplier_max = reward_config.improvement_multiplier_max
            env.improvement_multiplier_min = reward_config.improvement_multiplier_min
        except Exception as e:
            logger.log_warning(f"Failed to update environment {i} config: {e}")

    # Create progress tracker
    metrics_history = create_progress_tracker()
    consecutive_failures = 0
    best_episode_reward = float('-inf')

    # Get adaptive discriminator parameters
    adaptive_params = discriminator_monitor.get_adaptive_training_params()
    logger.log_info(
        f"ðŸ”§ Initial adaptive params - LR: {adaptive_params['discriminator_lr']:.2e}, Freq: {adaptive_params['update_frequency']}")

    # Main training loop with comprehensive discriminator monitoring
    for episode in range(num_episodes):
        episode_start_time = time.time()

        for env in envs:
            env.set_current_episode(episode)

        # === DISCRIMINATOR MONITORING PHASE ===

        # Regular discriminator validation
        if discriminator_monitor.should_validate(episode):
            logger.log_info(f"ðŸ” Validating discriminator at episode {episode}")

            # Detailed validation every N episodes
            detailed = (episode % discriminator_monitor.detailed_validation_frequency == 0) and episode > 0
            validation_results = discriminator_monitor.validate_performance(episode, detailed=detailed)

            # Check for degradation and handle if necessary
            rollback_occurred = discriminator_monitor.check_and_handle_degradation(episode)

            if rollback_occurred:
                logger.log_warning(f"ðŸ”„ Discriminator rollback occurred - resetting environments")
                # Reset all environments after rollback
                for env in envs:
                    try:
                        enhanced_reset_environment(env)
                    except Exception:
                        pass

            # Update best model backup if performance improved
            discriminator_monitor.update_best_model(episode)

            # Log discriminator status to TensorBoard
            tb_logger.log_discriminator_metrics(episode, validation_results)

        # Get updated adaptive parameters
        adaptive_params = discriminator_monitor.get_adaptive_training_params()
        current_disc_update_freq = adaptive_params['update_frequency']

        # Environment refresh
        if episode % environment_refresh_frequency == 0 and episode > 0:
            reset_count = 0
            for i, env in enumerate(envs):
                try:
                    enhanced_reset_environment(env)
                    reset_count += 1
                except Exception:
                    pass

            logger.log_environment_refresh(episode, env_manager.get_stats())

        # Pool refresh
        if (env_manager.should_refresh_pool(episode) or
                consecutive_failures > consecutive_failures_threshold):
            env_manager._refresh_pool()
            consecutive_failures = 0

        # === ROLLOUT COLLECTION ===
        rollout_data = collect_rollouts(envs, policy, training_config, device)

        if len(rollout_data['rewards']) == 0:
            consecutive_failures += 1
            continue

        # Extract metrics directly from step-level data
        episode_reward = np.mean(rollout_data['rewards'])
        metrics_history['episode_rewards'].append(episode_reward)

        if episode % 100 == 0 and episode > 0:  # Log every 100 episodes
            for i, env in enumerate(envs):
                try:
                    analysis = env.log_hub_score_analysis(detailed=True)
                    logger.log_info(f"ðŸŽ¯ Environment {i} Hub Score Analysis:\n{analysis}")
                except Exception as e:
                    logger.log_warning(f"Failed to log hub score analysis for env {i}: {e}")

        # Get step-level metrics
        step_infos = rollout_data.get('step_infos', [])
        rollout_hub_improvements = rollout_data.get('hub_improvements', [])
        rollout_hub_scores = rollout_data.get('hub_scores', [])
        episode_step_metrics = rollout_data.get('episode_metrics', {})

        # Calculate success rate from actual step improvements
        if rollout_hub_improvements:
            successful_improvements = [imp for imp in rollout_hub_improvements if imp > hub_score_improvement_threshold]
            success_rate = len(successful_improvements) / len(rollout_hub_improvements)
        else:
            success_rate = 0.0
        metrics_history['success_rates'].append(success_rate)

        # Calculate average improvements from step data
        avg_improvement = np.mean(rollout_hub_improvements) if rollout_hub_improvements else 0.0
        metrics_history['hub_improvements'].append(avg_improvement)

        # Calculate average hub scores from step data
        avg_hub_score = np.mean(rollout_hub_scores) if rollout_hub_scores else 0.5
        metrics_history['hub_scores'].append(avg_hub_score)

        # Update consecutive failures counter
        if success_rate > 0.05 or episode_reward > -0.5:
            consecutive_failures = 0
        else:
            consecutive_failures += 1

        # Track best performance
        if episode_reward > best_episode_reward:
            best_episode_reward = episode_reward

        # Intelligent episode logging
        episode_metrics = {
            'reward': episode_reward,
            'success_rate': success_rate,
            'hub_improvement': avg_improvement,
            'average_hub_score': avg_hub_score,
            'consecutive_failures': consecutive_failures
        }
        logger.log_episode_progress(episode, episode_metrics)

        # === POLICY UPDATES ===
        if episode % update_frequency == 0 and len(rollout_data['states']) >= 16:

            # Compute GAE advantages
            advantages, returns = trainer.compute_gae(
                rollout_data['rewards'],
                rollout_data['values'],
                rollout_data['dones']
            )

            # Update policy
            trainer.update_policy(
                rollout_data['states'],
                rollout_data['actions'],
                rollout_data['log_probs'],
                advantages,
                returns
            )

            # Log training metrics
            training_metrics = trainer.get_latest_metrics()
            logger.log_training_update(trainer.update_count, training_metrics)

            # Log policy metrics to TensorBoard
            if training_metrics:
                for key, value in training_metrics.items():
                    if 'policy' in key:
                        tb_logger.log_policy_metrics(trainer.update_count, {key.replace('policy_', ''): value})

        # === DISCRIMINATOR UPDATES (with adaptive frequency) ===
        if episode % current_disc_update_freq == 0 and episode > 100:
            # Skip if discriminator is critically degraded
            if not adaptive_params['should_skip_update']:
                try:
                    logger.log_info(
                        f"ðŸ§  Updating discriminator at episode {episode} (adaptive frequency: {current_disc_update_freq})")

                    smelly_samples, labels = data_loader.load_dataset(
                        max_samples_per_class=disc_config.max_samples_per_class,
                        shuffle=True,
                        remove_metadata=True
                    )

                    smelly_graphs = [data for data, label in zip(smelly_samples, labels) if label == 1]
                    clean_graphs = [data for data, label in zip(smelly_samples, labels) if label == 0]

                    recent_states = []
                    for env in envs:
                        if hasattr(env, 'current_data') and env.current_data is not None:
                            recent_states.append(env.current_data.clone())

                    min_samples = disc_config.min_samples_per_class
                    if (len(smelly_graphs) >= min_samples and
                            len(clean_graphs) >= min_samples and
                            len(recent_states) >= min_samples):

                        # Perform discriminator update with monitoring
                        trainer.update_discriminator(
                            smelly_graphs[:disc_config.max_samples_per_class],
                            clean_graphs[:disc_config.max_samples_per_class],
                            recent_states[:disc_config.max_samples_per_class],
                            episode
                        )

                        # Immediate post-update validation to check for degradation
                        post_update_validation = discriminator_monitor.validate_performance(
                            episode, detailed=False
                        )

                        # Check if update caused degradation
                        if discriminator_monitor.perf_state.is_critically_degraded():
                            logger.log_warning(f"ðŸš¨ Critical degradation detected after discriminator update!")
                            discriminator_monitor.check_and_handle_degradation(episode)

                        # Log to TensorBoard
                        disc_metrics = trainer.get_latest_metrics()
                        if disc_metrics:
                            for key, value in disc_metrics.items():
                                if 'discriminator' in key:
                                    tb_logger.log_discriminator_metrics(episode,
                                                                        {key.replace('discriminator_', ''): value})

                except Exception as e:
                    logger.log_warning(f"Discriminator update failed at episode {episode}: {e}")
            else:
                logger.log_warning(f"âš ï¸  Skipping discriminator update due to critical performance")

        # === TENSORBOARD LOGGING ===
        tb_logger.log_episode_metrics(episode, {
            'reward': episode_reward,
            'success_rate': success_rate,
            'hub_improvement': avg_improvement,
            'average_hub_score': avg_hub_score,
            'consecutive_failures': consecutive_failures,
            'best_reward_so_far': best_episode_reward
        })

        if episode % 50 == 0 and episode > 0:  # Every 50 episodes
            try:
                # Get breakdown from first environment as representative
                breakdown = envs[0].get_hub_score_breakdown()
                components = breakdown['component_scores']

                if components:
                    tb_logger.log_training_metrics(episode, {
                        'hub_score_gini': components.get('gini', 0.0),
                        'hub_score_degree_cent': components.get('degree_centralization', 0.0),
                        'hub_score_betweenness_cent': components.get('betweenness_centralization', 0.0),
                        'hub_score_discriminator': components.get('discriminator', 0.0),
                        'hub_score_composite': components.get('composite', 0.0)
                    })

                    # Log current weights
                    weights = breakdown['current_weights']
                    tb_logger.log_training_metrics(episode, {
                        'weight_gini': weights.get('gini_weight', 0.0),
                        'weight_degree_cent': weights.get('degree_centralization_weight', 0.0),
                        'weight_betweenness_cent': weights.get('betweenness_centralization_weight', 0.0),
                        'weight_discriminator': weights.get('discriminator_weight', 0.0)
                    })

            except Exception as e:
                logger.log_warning(f"Failed to log enhanced hub score metrics: {e}")

        # Log discriminator monitoring metrics
        monitoring_summary = discriminator_monitor.get_monitoring_summary()
        if episode % 25 == 0:
            perf_state = monitoring_summary['performance_state']
            tb_logger.log_discriminator_metrics(episode, {
                'adaptive_lr': adaptive_params['discriminator_lr'],
                'adaptive_update_freq': adaptive_params['update_frequency'],
                'degradation_count': perf_state['stability']['consecutive_degradations'],
                'improvement_count': perf_state['stability']['consecutive_improvements'],
                'vs_baseline_acc': perf_state['vs_baseline']['accuracy_diff'],
                'vs_baseline_f1': perf_state['vs_baseline']['f1_diff']
            })

        # === MILESTONE SUMMARIES ===
        logger.log_milestone_summary(episode, metrics_history)

        if episode % 250 == 0 and episode > 0:
            try:
                trend_analysis = analyze_enhanced_hub_score_trends(envs, episode, logger)

                if trend_analysis['status'] == 'success':
                    logger.log_info("ðŸ“ˆ ENHANCED HUB SCORE TREND ANALYSIS:")
                    logger.log_info(f"   Episode: {episode}")
                    logger.log_info(f"   Dominant Phase: {trend_analysis['dominant_phase']}")
                    logger.log_info(f"   Environments Analyzed: {trend_analysis['num_environments']}")

                    avg_comps = trend_analysis['average_components']
                    logger.log_info("   ðŸ“Š Average Component Scores:")
                    logger.log_info(
                        f"      Gini: {avg_comps.get('avg_gini', 0.0):.3f} Â± {avg_comps.get('std_gini', 0.0):.3f}")
                    logger.log_info(
                        f"      Degree Centralization: {avg_comps.get('avg_degree_centralization', 0.0):.3f} Â± {avg_comps.get('std_degree_centralization', 0.0):.3f}")
                    logger.log_info(
                        f"      Betweenness Centralization: {avg_comps.get('avg_betweenness_centralization', 0.0):.3f} Â± {avg_comps.get('std_betweenness_centralization', 0.0):.3f}")
                    logger.log_info(
                        f"      Discriminator: {avg_comps.get('avg_discriminator', 0.0):.3f} Â± {avg_comps.get('std_discriminator', 0.0):.3f}")
                    logger.log_info(
                        f"      Composite: {avg_comps.get('avg_composite', 0.0):.3f} Â± {avg_comps.get('std_composite', 0.0):.3f}")

                    current_weights = trend_analysis['current_weights']
                    logger.log_info("   âš–ï¸  Current Weights:")
                    logger.log_info(f"      Gini: {current_weights.get('gini_weight', 0.0):.2f}")
                    logger.log_info(
                        f"      Degree Cent: {current_weights.get('degree_centralization_weight', 0.0):.2f}")
                    logger.log_info(
                        f"      Betweenness Cent: {current_weights.get('betweenness_centralization_weight', 0.0):.2f}")
                    logger.log_info(f"      Discriminator: {current_weights.get('discriminator_weight', 0.0):.2f}")

            except Exception as e:
                logger.log_warning(f"Failed to perform trend analysis: {e}")

        # === CHECKPOINT SAVING ===
        if episode % 500 == 0 and episode > 0:
            try:
                # Include discriminator monitoring data
                discriminator_stats = discriminator_monitor.get_monitoring_summary()
                final_enhanced_analysis = analyze_enhanced_hub_score_trends(envs, num_episodes, logger)

                checkpoint = {
                    'episode': episode,
                    'policy_state': policy.state_dict(),
                    'discriminator_state': discriminator.state_dict(),
                    'trainer_state': {
                        'update_count': trainer.update_count,
                        'policy_losses': trainer.policy_losses[-100:],
                        'disc_losses': trainer.disc_losses[-100:],
                        'policy_metrics': trainer.policy_metrics[-50:],
                        'disc_metrics': trainer.disc_metrics[-50:]
                    },
                    'metrics_stats': {
                        'episode_rewards': metrics_history['episode_rewards'][-500:],
                        'success_rates': metrics_history['success_rates'][-500:],
                        'hub_improvements': metrics_history['hub_improvements'][-500:],
                        'hub_scores': metrics_history['hub_scores'][-500:],
                        'best_episode_reward': best_episode_reward,
                        'consecutive_failures': consecutive_failures
                    },
                    'discriminator_monitoring': {
                        'performance_state': discriminator_monitor.perf_state.get_status_summary(),
                        'monitoring_stats': discriminator_stats['monitoring_stats'],
                        'validation_history': discriminator_monitor.validation_history[-20:],  # Last 20 validations
                        'adaptive_params': adaptive_params,
                        'baseline_performance': discriminator_monitor.perf_state.baseline_accuracy
                    },
                    'centralized_config_used': {
                        'rewards': reward_config.__dict__,
                        'training': training_config.__dict__,
                        'optimization': opt_config.__dict__,
                        'discriminator': disc_config.__dict__,
                        'environment': env_config.__dict__
                    },
                    'enhanced_hub_score_analysis': {
                        'final_trend_analysis': final_enhanced_analysis,
                        'sample_environment_breakdown': envs[0].get_hub_score_breakdown() if envs else None,
                        'weight_evolution_summary': {
                            'early_weights': AdaptiveWeightScheduler().weight_configs['early'],
                            'mid_weights': AdaptiveWeightScheduler().weight_configs['mid'],
                            'late_weights': AdaptiveWeightScheduler().weight_configs['late'],
                            'final_phase': AdaptiveWeightScheduler().get_phase_name(episode)
                        }
                    }
                }

                checkpoint_path = results_dir / f'rl_checkpoint_{episode}.pt'
                torch.save(checkpoint, checkpoint_path)

                performance = {
                    'best_reward': best_episode_reward,
                    'current_success_rate': success_rate,
                    'discriminator_status': 'stable' if discriminator_monitor.perf_state.is_stable() else 'unstable'
                }
                logger.log_checkpoint_save(episode, checkpoint_path, performance)

                # Log discriminator checkpoint info
                logger.log_info(
                    f"ðŸ§  Discriminator state - Acc: {discriminator_monitor.perf_state.current_accuracy:.3f}, "
                    f"Interventions: {discriminator_monitor.emergency_interventions}, "
                    f"Rollbacks: {discriminator_monitor.rollback_count}")

            except Exception as e:
                logger.log_warning(f"Failed to save checkpoint: {e}")

        # === MEMORY CLEANUP ===
        if episode % 25 == 0 and device.type == 'cuda':
            torch.cuda.empty_cache()
            monitor_gpu_usage(logger)

        # === EMERGENCY DISCRIMINATOR INTERVENTIONS ===
        # Check for critical discriminator state every 50 episodes
        if episode % 50 == 0 and episode > 200:
            if discriminator_monitor.perf_state.is_critically_degraded():
                logger.log_warning(f"ðŸš¨ EMERGENCY: Critical discriminator degradation detected!")

                # Force validation and intervention
                emergency_validation = discriminator_monitor.validate_performance(episode, detailed=True)
                emergency_handled = discriminator_monitor.check_and_handle_degradation(episode)

                if emergency_handled:
                    logger.log_warning(f"ðŸ”„ Emergency intervention completed - restarting environments")
                    # Reset all environments after emergency intervention
                    for env in envs:
                        try:
                            enhanced_reset_environment(env)
                        except Exception:
                            pass

    # === TRAINING COMPLETION ===
    logger.log_info("RL Training completed!")

    # Final discriminator analysis
    logger.log_info("ðŸ” Performing final comprehensive discriminator validation...")
    final_discriminator_validation = discriminator_monitor.validate_performance(
        episode=num_episodes, detailed=True
    )

    # Final performance analysis
    if metrics_history['episode_rewards']:
        final_100_rewards = metrics_history['episode_rewards'][-100:] if len(
            metrics_history['episode_rewards']) >= 100 else metrics_history['episode_rewards']
        final_100_success = metrics_history['success_rates'][-100:] if len(metrics_history['success_rates']) >= 100 else \
            metrics_history['success_rates']
        final_100_improvements = metrics_history['hub_improvements'][-100:] if len(
            metrics_history['hub_improvements']) >= 100 else metrics_history['hub_improvements']
        final_100_hub_scores = metrics_history['hub_scores'][-100:] if len(metrics_history['hub_scores']) >= 100 else \
            metrics_history['hub_scores']

        final_metrics = {
            'mean_reward_last_100': float(np.mean(final_100_rewards)),
            'mean_success_rate_last_100': float(np.mean(final_100_success)),
            'mean_hub_improvement_last_100': float(np.mean(final_100_improvements)),
            'mean_hub_score_last_100': float(np.mean(final_100_hub_scores)),
            'total_updates': trainer.update_count
        }

        # Include discriminator final analysis
        discriminator_final_summary = discriminator_monitor.get_monitoring_summary()

        logger.log_final_summary(num_episodes, {
            'final_performance': final_metrics,
            'discriminator_analysis': discriminator_final_summary
        })

    # === FINAL MODEL SAVING ===
    try:
        final_save_path = results_dir / 'final_rl_model.pt'
        final_stats = {
            'total_episodes': num_episodes,
            'best_episode_reward': best_episode_reward,
            'final_performance': final_metrics if 'final_metrics' in locals() else {},
            'consecutive_failures': consecutive_failures,
            'total_updates': trainer.update_count
        }

        # Comprehensive discriminator statistics for final save
        discriminator_final_stats = {
            'final_validation': final_discriminator_validation,
            'monitoring_summary': discriminator_monitor.get_monitoring_summary(),
            'performance_evolution': {
                'baseline': {
                    'accuracy': discriminator_monitor.perf_state.baseline_accuracy,
                    'f1': discriminator_monitor.perf_state.baseline_f1
                },
                'final': {
                    'accuracy': discriminator_monitor.perf_state.current_accuracy,
                    'f1': discriminator_monitor.perf_state.current_f1
                },
                'best': {
                    'accuracy': discriminator_monitor.perf_state.best_accuracy,
                    'f1': discriminator_monitor.perf_state.best_f1,
                    'episode': discriminator_monitor.perf_state.best_episode
                }
            },
            'interventions': {
                'emergency_interventions': discriminator_monitor.emergency_interventions,
                'rollbacks': discriminator_monitor.rollback_count,
                'total_validations': len(discriminator_monitor.validation_history),
                'failed_validations': discriminator_monitor.perf_state.failed_validations
            },
            'adaptive_learning': {
                'final_lr': adaptive_params['discriminator_lr'],
                'final_update_freq': adaptive_params['update_frequency'],
                'lr_adjustments': discriminator_monitor.current_discriminator_lr != discriminator_monitor.base_discriminator_lr
            }
        }

        torch.save({
            'policy_state': policy.state_dict(),
            'discriminator_state': discriminator.state_dict(),
            'node_dim': 7,
            'edge_dim': 1,
            'training_stats': final_stats,
            'training_history': metrics_history,
            'trainer_final_metrics': trainer.get_latest_metrics(),
            'discriminator_comprehensive_analysis': discriminator_final_stats,
            'centralized_config': {
                'rewards': reward_config.__dict__,
                'training': training_config.__dict__,
                'optimization': opt_config.__dict__,
                'discriminator': disc_config.__dict__,
                'environment': env_config.__dict__
            }
        }, final_save_path)

        logger.log_info(f"Final model saved: {final_save_path}")

        # Log final discriminator status
        logger.log_info("ðŸ§  FINAL DISCRIMINATOR ANALYSIS:")
        logger.log_info(f"   Baseline Performance: Acc {discriminator_monitor.perf_state.baseline_accuracy:.3f}")
        logger.log_info(f"   Final Performance: Acc {discriminator_monitor.perf_state.current_accuracy:.3f}")
        logger.log_info(
            f"   Best Performance: Acc {discriminator_monitor.perf_state.best_accuracy:.3f} (Episode {discriminator_monitor.perf_state.best_episode})")
        logger.log_info(f"   Emergency Interventions: {discriminator_monitor.emergency_interventions}")
        logger.log_info(f"   Model Rollbacks: {discriminator_monitor.rollback_count}")
        logger.log_info(f"   Total Validations: {len(discriminator_monitor.validation_history)}")

        # Performance verdict
        acc_change = discriminator_monitor.perf_state.current_accuracy - discriminator_monitor.perf_state.baseline_accuracy
        if acc_change > 0.05:
            verdict = "âœ… SIGNIFICANTLY IMPROVED"
        elif acc_change > 0.02:
            verdict = "âœ… IMPROVED"
        elif acc_change > -0.02:
            verdict = "âž¡ï¸  MAINTAINED"
        elif acc_change > -0.05:
            verdict = "âš ï¸  SLIGHTLY DEGRADED"
        else:
            verdict = "âŒ SIGNIFICANTLY DEGRADED"

        logger.log_info(f"   Overall Verdict: {verdict} ({acc_change:+.3f})")

    except Exception as e:
        logger.log_error(f"Failed to save final model: {e}")

    finally:
        tb_logger.close()

    # Final recommendations based on discriminator behavior
    logger.log_info("\n" + "=" * 70)
    logger.log_info("ðŸŽ¯ TRAINING RECOMMENDATIONS BASED ON DISCRIMINATOR ANALYSIS:")
    logger.log_info("=" * 70)

    if discriminator_monitor.emergency_interventions > 3:
        logger.log_info("âš ï¸  High number of emergency interventions - consider:")
        logger.log_info("   â€¢ Lower discriminator learning rate")
        logger.log_info("   â€¢ Increase discriminator update frequency")
        logger.log_info("   â€¢ Review discriminator architecture complexity")

    if discriminator_monitor.rollback_count > 1:
        logger.log_info("ðŸ”„ Multiple rollbacks occurred - consider:")
        logger.log_info("   â€¢ More conservative discriminator updates")
        logger.log_info("   â€¢ Better discriminator regularization")
        logger.log_info("   â€¢ Longer baseline establishment period")

    if discriminator_monitor.perf_state.is_stable():
        logger.log_info("âœ… Discriminator remained stable throughout training")
        logger.log_info("   â€¢ Current hyperparameters appear well-tuned")
        logger.log_info("   â€¢ Consider longer training for better results")
    else:
        logger.log_info("âš ï¸  Discriminator showed instability - consider:")
        logger.log_info("   â€¢ Review adversarial training balance")
        logger.log_info("   â€¢ Implement more frequent validation")
        logger.log_info("   â€¢ Adjust policy-discriminator update ratio")

    logger.log_info("=" * 70)
    logger.log_info("ðŸ COMPREHENSIVE RL TRAINING WITH DISCRIMINATOR MONITORING COMPLETED!")
    logger.log_info("=" * 70)
    logger.log_info(f"ðŸ“Š View detailed training progress: tensorboard --logdir {tensorboard_dir}")
    logger.log_info(f"âš™ï¸  Edit hyperparameters: hyperparameters_configuration.py")
    logger.log_info(f"ðŸ“ Results saved in: {results_dir}")


if __name__ == "__main__":
    main()