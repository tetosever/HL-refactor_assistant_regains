# Configurazione aggiornata per Graph Refactoring RL con patch reward shaping
# Implementa le modifiche richieste per hub_score e reward shaping

environment:
  data_path: "data_builder/dataset/graph_features"
  max_episode_steps: 20
  reward_weights:
    hub_score: 0.0              # *** PATCH: Alpha per reward shaping ***
    step_valid: 0.01             # *** PATCH: Ridotto da 0.1 ***
    step_invalid: -0.01          # *** PATCH: Meno penalizzante ***
    cycle_penalty: -0.05         # *** PATCH: Molto ridotto ***
    duplicate_penalty: -0.02     # *** PATCH: Molto ridotto ***
    time_penalty: -0.01          # *** PATCH: NUOVO - penalty per tempo ***
    adversarial_weight: 0.1     # *** PATCH: Riattivato, basso ***
    terminal_thresh: 0.01        # *** PATCH: Soglia più sensibile ***
    terminal_bonus: 0.0          # *** PATCH: Bonus aumentato ***
    patience: 8              # *** PATCH: NUOVO - early termination ***

model:
  node_dim: 7
  hidden_dim: 128
  num_layers: 3
  num_actions: 7             # Fix: Confermato 7 azioni (include STOP)
  global_features_dim: 10
  dropout: 0.2
  shared_encoder: false      # Fix: Encoder separati per Actor/Critic

optimization:
  gamma: 0.99                # *** PATCH: Aumentato per propagazione bonus terminale ***
  lr_actor: 0.0003          # Mantenuto 3e-4
  lr_critic: 0.0001         # Mantenuto 1e-4
  lr_discriminator: 0.0001  # Mantenuto 1e-4
  value_loss_coef: 0.5
  entropy_coef: 0.08        # *** PATCH: Leggermente aumentato ***
  max_grad_norm: 0.5
  weight_decay: 0.00001

schedule:
  num_episodes: 2000        # Mantenuto per test rapidi
  warmup_episodes: 600      # Aumentato per stabilità
  adversarial_start_episode: 600  # *** PATCH: Starts at warmup end ***
  batch_size: 64            # *** PATCH: Aumentato per stabilità ***
  update_every: 15          # *** PATCH: Aggiornamenti più frequenti ***
  discriminator_update_every: 50  # *** PATCH: Aggiornamenti più frequenti ***

logging:
  log_every: 25
  eval_every: 100
  save_every: 100
  num_eval_episodes: 20
  early_stopping_patience: 100
  min_improvement: 0.005

# *** PATCH: Sezione di configurazione per il nuovo hub_score ***
hub_score_config:
  # Pesi per la combinazione gini + share (devono sommare a 1.0)
  w1_gini: 0.5               # Peso per gini coefficient
  w2_share: 0.5              # Peso per share metric

  # Normalizzazione e clipping
  normalize_range: [0.0, 1.0]  # Range di normalizzazione

  # Documentazione dei test
  expected_behavior:
    delta_positive_improvement: "prev=0.7, current=0.5 → delta=+0.2 → reward positivo"
    best_so_far_tracking: "Usa il miglior hub_score dell'episodio per final reward"
    discriminator_delta: "disc_start=0.8, p_now=0.6 → disc_delta=+0.2 → reward positivo"
    stop_action_neutral: "action==6 (STOP) non riceve step_valid reward"
    time_penalty_applied: "Ogni step riceve time_penalty (tipicamente negativo)"

# *** PATCH: Test di validazione per le modifiche ***
validation_tests:
  test_hub_score_normalization:
    description: "Hub score deve essere in [0,1]"
    expected_range: [0.0, 1.0]

  test_reward_shaping_sign:
    description: "Delta positivo deve dare reward positivo"
    test_case: "prev=0.7, current=0.5 → alpha * (+0.2) > 0"

  test_best_so_far_tracking:
    description: "Final reward usa best hub score dell'episodio"
    scenario: "Scendi a 0.45 poi risali a 0.52 → usa 0.45"

  test_discriminator_improvement:
    description: "Riduzione probabilità smelly = reward positivo"
    test_case: "disc_start=0.8, p_now=0.6 → +0.2 * adv_weight > 0"

  test_stop_action_neutral:
    description: "Action STOP non riceve step_valid"
    expected: "step_reward = 0.0 quando action == 6"