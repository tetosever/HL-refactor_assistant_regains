{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T22:36:41.553684Z",
     "start_time": "2025-08-10T22:36:37.835917Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "from rl_gym import RefactorEnv\n",
    "from actor_critic_models import create_actor_critic\n",
    "from discriminator import create_discriminator\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 Graph Refactoring RL - Evaluation Dashboard\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch_geometric\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Data\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch_geometric\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m to_networkx\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplotly\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgraph_objects\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgo\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplotly\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexpress\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpx\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplotly\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msubplots\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m make_subplots\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'plotly'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EvaluationConfig:\n",
    "    \"\"\"Configuration for evaluation\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.model_path = \"results/rl_training/best_model.pt\"\n",
    "        self.discriminator_path = \"results/discriminator_pretraining/pretrained_discriminator.pt\"\n",
    "        self.data_path = \"data_builder/dataset/graph_features\"\n",
    "        self.results_dir = \"results/evaluation\"\n",
    "\n",
    "        # Evaluation parameters\n",
    "        self.num_eval_episodes = 50\n",
    "        self.num_visualization_episodes = 5\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # Visualization parameters\n",
    "        self.figure_size = (15, 10)\n",
    "        self.node_size_factor = 300\n",
    "        self.edge_width = 2.0\n",
    "        self.colors = {\n",
    "            'original_nodes': '#3498db',\n",
    "            'original_edges': '#2c3e50',\n",
    "            'added_nodes': '#e74c3c',\n",
    "            'added_edges': '#e74c3c',\n",
    "            'removed_edges': '#95a5a6',\n",
    "            'hub_node': '#f39c12',\n",
    "            'modified_nodes': '#9b59b6'\n",
    "        }\n",
    "\n",
    "eval_config = EvaluationConfig()"
   ],
   "id": "6cfe7e3160652697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_trained_model(model_path: str, device: str) -> Tuple[torch.nn.Module, Dict]:\n",
    "    \"\"\"Load trained RL model\"\"\"\n",
    "\n",
    "    print(f\"📦 Loading model from {model_path}\")\n",
    "\n",
    "    if not Path(model_path).exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "    # Get model configuration\n",
    "    if 'model_config' in checkpoint:\n",
    "        model_config = checkpoint['model_config']\n",
    "    else:\n",
    "        # Fallback configuration\n",
    "        model_config = {\n",
    "            'node_dim': 7,\n",
    "            'hidden_dim': 128,\n",
    "            'num_layers': 3,\n",
    "            'num_actions': 7,\n",
    "            'global_features_dim': 4,\n",
    "            'dropout': 0.2,\n",
    "            'shared_encoder': False\n",
    "        }\n",
    "        print(\"⚠️ Using fallback model configuration\")\n",
    "\n",
    "    # Create and load model\n",
    "    model = create_actor_critic(model_config).to(device)\n",
    "    model.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"✅ Model loaded successfully\")\n",
    "    print(f\"   Architecture: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "    return model, checkpoint"
   ],
   "id": "3d56682b46485f6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_discriminator(discriminator_path: str, device: str) -> Optional[torch.nn.Module]:\n",
    "    \"\"\"Load pre-trained discriminator\"\"\"\n",
    "\n",
    "    if not Path(discriminator_path).exists():\n",
    "        print(f\"⚠️ Discriminator not found: {discriminator_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(discriminator_path, map_location=device)\n",
    "        model_config = checkpoint['model_config']\n",
    "\n",
    "        discriminator = create_discriminator(**model_config)\n",
    "        discriminator.load_state_dict(checkpoint['model_state_dict'])\n",
    "        discriminator.to(device)\n",
    "        discriminator.eval()\n",
    "\n",
    "        print(f\"✅ Discriminator loaded successfully\")\n",
    "        return discriminator\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load discriminator: {e}\")\n",
    "        return None"
   ],
   "id": "2cc986f2cf5c0858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GraphRefactoringEvaluator:\n",
    "    \"\"\"Comprehensive evaluator for graph refactoring RL\"\"\"\n",
    "\n",
    "    def __init__(self, model, discriminator, config: EvaluationConfig):\n",
    "        self.model = model\n",
    "        self.discriminator = discriminator\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "\n",
    "        # Initialize environment\n",
    "        self.env = RefactorEnv(\n",
    "            data_path=config.data_path,\n",
    "            discriminator=discriminator,\n",
    "            max_steps=20,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        # Results storage\n",
    "        self.evaluation_results = []\n",
    "        self.episode_trajectories = []\n",
    "\n",
    "    def run_single_episode(self, episode_id: int, save_trajectory: bool = False) -> Dict:\n",
    "        \"\"\"Run a single evaluation episode\"\"\"\n",
    "\n",
    "        # Reset environment\n",
    "        self.env.reset()\n",
    "        initial_data = self.env.current_data.clone()\n",
    "        initial_metrics = self.env._calculate_metrics(initial_data)\n",
    "\n",
    "        # Episode tracking\n",
    "        episode_data = {\n",
    "            'episode_id': episode_id,\n",
    "            'initial_hub_score': initial_metrics['hub_score'],\n",
    "            'initial_metrics': initial_metrics,\n",
    "            'initial_graph': initial_data,\n",
    "            'actions_taken': [],\n",
    "            'states': [],\n",
    "            'rewards': [],\n",
    "            'step_info': []\n",
    "        }\n",
    "\n",
    "        if save_trajectory:\n",
    "            episode_data['states'].append(initial_data.clone())\n",
    "\n",
    "        # Get initial discriminator score\n",
    "        initial_disc_score = None\n",
    "        if self.discriminator is not None:\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    disc_output = self.discriminator(initial_data)\n",
    "                    if isinstance(disc_output, dict):\n",
    "                        initial_disc_score = torch.softmax(disc_output['logits'], dim=1)[0, 1].item()\n",
    "                    else:\n",
    "                        initial_disc_score = torch.softmax(disc_output, dim=1)[0, 1].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        episode_data['initial_disc_score'] = initial_disc_score\n",
    "\n",
    "        # Run episode\n",
    "        episode_reward = 0.0\n",
    "        episode_length = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            current_data = self.env.current_data\n",
    "            global_features = self.env._extract_global_features(current_data)\n",
    "\n",
    "            # Get action from model (greedy evaluation)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(current_data, global_features)\n",
    "                action_probs = torch.softmax(output['action_logits'], dim=1)\n",
    "                action = torch.argmax(action_probs, dim=1).item()\n",
    "                confidence = action_probs[0, action].item()\n",
    "\n",
    "            # Take action\n",
    "            _, reward, done, info = self.env.step(action)\n",
    "\n",
    "            # Record step\n",
    "            episode_data['actions_taken'].append(action)\n",
    "            episode_data['rewards'].append(reward)\n",
    "            episode_data['step_info'].append({\n",
    "                'action': action,\n",
    "                'confidence': confidence,\n",
    "                'reward': reward,\n",
    "                'done': done,\n",
    "                'action_success': info.get('action_success', False),\n",
    "                'hub_score': info.get('metrics', {}).get('hub_score', 0.0)\n",
    "            })\n",
    "\n",
    "            if save_trajectory:\n",
    "                episode_data['states'].append(self.env.current_data.clone())\n",
    "\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "\n",
    "        # Final metrics\n",
    "        final_data = self.env.current_data\n",
    "        final_metrics = self.env._calculate_metrics(final_data)\n",
    "\n",
    "        # Get final discriminator score\n",
    "        final_disc_score = None\n",
    "        if self.discriminator is not None:\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    disc_output = self.discriminator(final_data)\n",
    "                    if isinstance(disc_output, dict):\n",
    "                        final_disc_score = torch.softmax(disc_output['logits'], dim=1)[0, 1].item()\n",
    "                    else:\n",
    "                        final_disc_score = torch.softmax(disc_output, dim=1)[0, 1].item()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # Compile episode results\n",
    "        hub_improvement = initial_metrics['hub_score'] - final_metrics['hub_score']\n",
    "        disc_improvement = 0.0\n",
    "        if initial_disc_score is not None and final_disc_score is not None:\n",
    "            disc_improvement = initial_disc_score - final_disc_score\n",
    "\n",
    "        episode_data.update({\n",
    "            'final_hub_score': final_metrics['hub_score'],\n",
    "            'final_metrics': final_metrics,\n",
    "            'final_graph': final_data,\n",
    "            'final_disc_score': final_disc_score,\n",
    "            'episode_reward': episode_reward,\n",
    "            'episode_length': episode_length,\n",
    "            'hub_improvement': hub_improvement,\n",
    "            'disc_improvement': disc_improvement,\n",
    "            'success': hub_improvement > 0.01,  # Success threshold\n",
    "            'significant_success': hub_improvement > 0.05,\n",
    "            'num_valid_actions': sum(1 for step in episode_data['step_info'] if step['action_success']),\n",
    "            'final_action_was_stop': episode_data['actions_taken'][-1] == 6 if episode_data['actions_taken'] else False\n",
    "        })\n",
    "\n",
    "        return episode_data\n",
    "\n",
    "    def run_comprehensive_evaluation(self) -> Dict:\n",
    "        \"\"\"Run comprehensive evaluation\"\"\"\n",
    "\n",
    "        print(f\"🔄 Running evaluation on {self.config.num_eval_episodes} episodes...\")\n",
    "\n",
    "        # Run episodes\n",
    "        for episode_id in range(self.config.num_eval_episodes):\n",
    "            save_trajectory = episode_id < self.config.num_visualization_episodes\n",
    "\n",
    "            episode_result = self.run_single_episode(episode_id, save_trajectory)\n",
    "            self.evaluation_results.append(episode_result)\n",
    "\n",
    "            if save_trajectory:\n",
    "                self.episode_trajectories.append(episode_result)\n",
    "\n",
    "            # Progress update\n",
    "            if (episode_id + 1) % 10 == 0:\n",
    "                print(f\"   Completed {episode_id + 1}/{self.config.num_eval_episodes} episodes\")\n",
    "\n",
    "        # Compute summary statistics\n",
    "        summary_stats = self._compute_summary_statistics()\n",
    "\n",
    "        print(\"✅ Evaluation completed!\")\n",
    "        self._print_summary_stats(summary_stats)\n",
    "\n",
    "        return {\n",
    "            'summary_stats': summary_stats,\n",
    "            'episode_results': self.evaluation_results,\n",
    "            'trajectories': self.episode_trajectories\n",
    "        }\n",
    "\n",
    "    def _compute_summary_statistics(self) -> Dict:\n",
    "        \"\"\"Compute comprehensive summary statistics\"\"\"\n",
    "\n",
    "        results = self.evaluation_results\n",
    "\n",
    "        # Basic metrics\n",
    "        episode_rewards = [r['episode_reward'] for r in results]\n",
    "        hub_improvements = [r['hub_improvement'] for r in results]\n",
    "        episode_lengths = [r['episode_length'] for r in results]\n",
    "        disc_improvements = [r['disc_improvement'] for r in results if r['disc_improvement'] is not None]\n",
    "\n",
    "        # Success metrics\n",
    "        successes = [r['success'] for r in results]\n",
    "        significant_successes = [r['significant_success'] for r in results]\n",
    "\n",
    "        # Action analysis\n",
    "        all_actions = []\n",
    "        action_success_rates = {}\n",
    "        for r in results:\n",
    "            all_actions.extend(r['actions_taken'])\n",
    "            for step in r['step_info']:\n",
    "                action = step['action']\n",
    "                if action not in action_success_rates:\n",
    "                    action_success_rates[action] = {'total': 0, 'successful': 0}\n",
    "                action_success_rates[action]['total'] += 1\n",
    "                if step['action_success']:\n",
    "                    action_success_rates[action]['successful'] += 1\n",
    "\n",
    "        action_distribution = {}\n",
    "        for action in range(7):\n",
    "            action_distribution[action] = all_actions.count(action) / len(all_actions) if all_actions else 0\n",
    "\n",
    "        for action in action_success_rates:\n",
    "            action_success_rates[action]['rate'] = (\n",
    "                action_success_rates[action]['successful'] / action_success_rates[action]['total']\n",
    "                if action_success_rates[action]['total'] > 0 else 0\n",
    "            )\n",
    "\n",
    "        summary = {\n",
    "            # Basic performance\n",
    "            'num_episodes': len(results),\n",
    "            'success_rate': np.mean(successes),\n",
    "            'significant_success_rate': np.mean(significant_successes),\n",
    "\n",
    "            # Reward statistics\n",
    "            'mean_episode_reward': np.mean(episode_rewards),\n",
    "            'std_episode_reward': np.std(episode_rewards),\n",
    "            'median_episode_reward': np.median(episode_rewards),\n",
    "\n",
    "            # Hub improvement statistics\n",
    "            'mean_hub_improvement': np.mean(hub_improvements),\n",
    "            'std_hub_improvement': np.std(hub_improvements),\n",
    "            'median_hub_improvement': np.median(hub_improvements),\n",
    "            'max_hub_improvement': np.max(hub_improvements),\n",
    "            'min_hub_improvement': np.min(hub_improvements),\n",
    "\n",
    "            # Episode length statistics\n",
    "            'mean_episode_length': np.mean(episode_lengths),\n",
    "            'std_episode_length': np.std(episode_lengths),\n",
    "            'median_episode_length': np.median(episode_lengths),\n",
    "\n",
    "            # Discriminator statistics\n",
    "            'discriminator_available': len(disc_improvements) > 0,\n",
    "            'mean_disc_improvement': np.mean(disc_improvements) if disc_improvements else 0.0,\n",
    "            'std_disc_improvement': np.std(disc_improvements) if disc_improvements else 0.0,\n",
    "\n",
    "            # Action analysis\n",
    "            'action_distribution': action_distribution,\n",
    "            'action_success_rates': action_success_rates,\n",
    "            'total_actions_taken': len(all_actions),\n",
    "\n",
    "            # Performance bins\n",
    "            'excellent_performance': sum(1 for imp in hub_improvements if imp > 0.1) / len(hub_improvements),\n",
    "            'good_performance': sum(1 for imp in hub_improvements if 0.05 < imp <= 0.1) / len(hub_improvements),\n",
    "            'moderate_performance': sum(1 for imp in hub_improvements if 0.01 < imp <= 0.05) / len(hub_improvements),\n",
    "            'poor_performance': sum(1 for imp in hub_improvements if imp <= 0.01) / len(hub_improvements),\n",
    "        }\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def _print_summary_stats(self, stats: Dict):\n",
    "        \"\"\"Print formatted summary statistics\"\"\"\n",
    "\n",
    "        print(\"\\n📊 EVALUATION SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        print(f\"📈 SUCCESS METRICS:\")\n",
    "        print(f\"   Overall Success Rate: {stats['success_rate']:.1%}\")\n",
    "        print(f\"   Significant Success Rate: {stats['significant_success_rate']:.1%}\")\n",
    "        print(f\"   Excellent Performance (>0.1): {stats['excellent_performance']:.1%}\")\n",
    "        print(f\"   Good Performance (0.05-0.1): {stats['good_performance']:.1%}\")\n",
    "\n",
    "        print(f\"\\n🎯 HUB IMPROVEMENT:\")\n",
    "        print(f\"   Mean: {stats['mean_hub_improvement']:.4f} ± {stats['std_hub_improvement']:.4f}\")\n",
    "        print(f\"   Median: {stats['median_hub_improvement']:.4f}\")\n",
    "        print(f\"   Best: {stats['max_hub_improvement']:.4f}\")\n",
    "        print(f\"   Worst: {stats['min_hub_improvement']:.4f}\")\n",
    "\n",
    "        print(f\"\\n🏆 EPISODE REWARDS:\")\n",
    "        print(f\"   Mean: {stats['mean_episode_reward']:.3f} ± {stats['std_episode_reward']:.3f}\")\n",
    "        print(f\"   Median: {stats['median_episode_reward']:.3f}\")\n",
    "\n",
    "        print(f\"\\n📏 EPISODE LENGTH:\")\n",
    "        print(f\"   Mean: {stats['mean_episode_length']:.1f} ± {stats['std_episode_length']:.1f}\")\n",
    "        print(f\"   Median: {stats['median_episode_length']:.1f}\")\n",
    "\n",
    "        if stats['discriminator_available']:\n",
    "            print(f\"\\n🎭 DISCRIMINATOR IMPROVEMENT:\")\n",
    "            print(f\"   Mean: {stats['mean_disc_improvement']:.4f} ± {stats['std_disc_improvement']:.4f}\")\n",
    "\n",
    "        print(f\"\\n🎮 ACTION USAGE:\")\n",
    "        action_names = ['RemoveEdge', 'AddEdge', 'MoveEdge', 'ExtractMethod',\n",
    "                       'ExtractAbstractUnit', 'ExtractUnit', 'STOP']\n",
    "        for action, name in enumerate(action_names):\n",
    "            if action in stats['action_distribution']:\n",
    "                freq = stats['action_distribution'][action]\n",
    "                success_rate = stats['action_success_rates'].get(action, {}).get('rate', 0)\n",
    "                print(f\"   {name}: {freq:.1%} usage, {success_rate:.1%} success\")\n"
   ],
   "id": "6995327972678db4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_performance_dashboard(evaluation_results: Dict) -> None:\n",
    "    \"\"\"Create comprehensive performance dashboard\"\"\"\n",
    "\n",
    "    results = evaluation_results['episode_results']\n",
    "    stats = evaluation_results['summary_stats']\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "    # 1. Hub Improvement Distribution\n",
    "    plt.subplot(3, 4, 1)\n",
    "    hub_improvements = [r['hub_improvement'] for r in results]\n",
    "    plt.hist(hub_improvements, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='No improvement')\n",
    "    plt.axvline(np.mean(hub_improvements), color='green', linestyle='-', alpha=0.7, label='Mean')\n",
    "    plt.xlabel('Hub Score Improvement')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Hub Improvement Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Success Rate by Episode Length\n",
    "    plt.subplot(3, 4, 2)\n",
    "    lengths = [r['episode_length'] for r in results]\n",
    "    successes = [r['success'] for r in results]\n",
    "\n",
    "    length_bins = np.arange(1, max(lengths) + 2)\n",
    "    success_by_length = []\n",
    "    for length in length_bins[:-1]:\n",
    "        episodes_at_length = [s for l, s in zip(lengths, successes) if l == length]\n",
    "        if episodes_at_length:\n",
    "            success_by_length.append(np.mean(episodes_at_length))\n",
    "        else:\n",
    "            success_by_length.append(0)\n",
    "\n",
    "    plt.bar(length_bins[:-1], success_by_length, alpha=0.7, color='lightgreen')\n",
    "    plt.xlabel('Episode Length')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.title('Success Rate by Episode Length')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Episode Rewards Over Time\n",
    "    plt.subplot(3, 4, 3)\n",
    "    episode_rewards = [r['episode_reward'] for r in results]\n",
    "    plt.plot(episode_rewards, alpha=0.7, color='blue', linewidth=1)\n",
    "    plt.axhline(np.mean(episode_rewards), color='red', linestyle='--', alpha=0.7, label='Mean')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Action Distribution\n",
    "    plt.subplot(3, 4, 4)\n",
    "    action_names = ['RemoveEdge', 'AddEdge', 'MoveEdge', 'ExtractMethod',\n",
    "                   'ExtractAbstractUnit', 'ExtractUnit', 'STOP']\n",
    "    action_counts = [stats['action_distribution'].get(i, 0) for i in range(7)]\n",
    "\n",
    "    bars = plt.bar(range(7), action_counts, alpha=0.7, color='orange')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Usage Frequency')\n",
    "    plt.title('Action Usage Distribution')\n",
    "    plt.xticks(range(7), [name[:8] for name in action_names], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add percentage labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, action_counts)):\n",
    "        if count > 0:\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                    f'{count:.1%}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # 5. Hub Score: Before vs After\n",
    "    plt.subplot(3, 4, 5)\n",
    "    initial_scores = [r['initial_hub_score'] for r in results]\n",
    "    final_scores = [r['final_hub_score'] for r in results]\n",
    "\n",
    "    plt.scatter(initial_scores, final_scores, alpha=0.6, color='purple')\n",
    "    plt.plot([min(initial_scores), max(initial_scores)],\n",
    "             [min(initial_scores), max(initial_scores)],\n",
    "             'r--', alpha=0.7, label='No change')\n",
    "    plt.xlabel('Initial Hub Score')\n",
    "    plt.ylabel('Final Hub Score')\n",
    "    plt.title('Hub Score: Before vs After')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Performance Categories Pie Chart\n",
    "    plt.subplot(3, 4, 6)\n",
    "    categories = ['Excellent (>0.1)', 'Good (0.05-0.1)', 'Moderate (0.01-0.05)', 'Poor (≤0.01)']\n",
    "    values = [stats['excellent_performance'], stats['good_performance'],\n",
    "              stats['moderate_performance'], stats['poor_performance']]\n",
    "    colors = ['#2ecc71', '#f39c12', '#e67e22', '#e74c3c']\n",
    "\n",
    "    plt.pie(values, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Performance Categories')\n",
    "\n",
    "    # 7. Action Success Rates\n",
    "    plt.subplot(3, 4, 7)\n",
    "    success_rates = [stats['action_success_rates'].get(i, {}).get('rate', 0) for i in range(7)]\n",
    "\n",
    "    bars = plt.bar(range(7), success_rates, alpha=0.7, color='lightcoral')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.title('Action Success Rates')\n",
    "    plt.xticks(range(7), [name[:8] for name in action_names], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add percentage labels\n",
    "    for i, (bar, rate) in enumerate(zip(bars, success_rates)):\n",
    "        if rate > 0:\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{rate:.1%}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # 8. Cumulative Success Rate\n",
    "    plt.subplot(3, 4, 8)\n",
    "    cumulative_successes = np.cumsum([r['success'] for r in results])\n",
    "    cumulative_rate = cumulative_successes / np.arange(1, len(results) + 1)\n",
    "\n",
    "    plt.plot(cumulative_rate, color='green', linewidth=2)\n",
    "    plt.axhline(stats['success_rate'], color='red', linestyle='--', alpha=0.7,\n",
    "                label=f'Final: {stats[\"success_rate\"]:.1%}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Success Rate')\n",
    "    plt.title('Cumulative Success Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 9. Discriminator Improvement (if available)\n",
    "    if stats['discriminator_available']:\n",
    "        plt.subplot(3, 4, 9)\n",
    "        disc_improvements = [r['disc_improvement'] for r in results if r['disc_improvement'] is not None]\n",
    "        plt.hist(disc_improvements, bins=15, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "        plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='No improvement')\n",
    "        plt.axvline(np.mean(disc_improvements), color='green', linestyle='-', alpha=0.7, label='Mean')\n",
    "        plt.xlabel('Discriminator Score Improvement')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Discriminator Improvement')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 10. Hub Improvement vs Episode Reward\n",
    "    plt.subplot(3, 4, 10)\n",
    "    rewards = [r['episode_reward'] for r in results]\n",
    "    improvements = [r['hub_improvement'] for r in results]\n",
    "\n",
    "    plt.scatter(improvements, rewards, alpha=0.6, color='teal')\n",
    "    plt.xlabel('Hub Score Improvement')\n",
    "    plt.ylabel('Episode Reward')\n",
    "    plt.title('Hub Improvement vs Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 11. Episode Length Distribution\n",
    "    plt.subplot(3, 4, 11)\n",
    "    plt.hist(lengths, bins=range(1, max(lengths) + 2), alpha=0.7, color='gold', edgecolor='black')\n",
    "    plt.axvline(np.mean(lengths), color='red', linestyle='--', alpha=0.7, label=f'Mean: {np.mean(lengths):.1f}')\n",
    "    plt.xlabel('Episode Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Episode Length Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 12. Success Rate by Hub Improvement Range\n",
    "    plt.subplot(3, 4, 12)\n",
    "    improvement_ranges = ['<0', '0-0.01', '0.01-0.05', '0.05-0.1', '>0.1']\n",
    "    range_counts = [\n",
    "        sum(1 for imp in hub_improvements if imp < 0),\n",
    "        sum(1 for imp in hub_improvements if 0 <= imp <= 0.01),\n",
    "        sum(1 for imp in hub_improvements if 0.01 < imp <= 0.05),\n",
    "        sum(1 for imp in hub_improvements if 0.05 < imp <= 0.1),\n",
    "        sum(1 for imp in hub_improvements if imp > 0.1)\n",
    "    ]\n",
    "\n",
    "    bars = plt.bar(improvement_ranges, range_counts, alpha=0.7, color='mediumpurple')\n",
    "    plt.xlabel('Hub Improvement Range')\n",
    "    plt.ylabel('Number of Episodes')\n",
    "    plt.title('Episodes by Improvement Range')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars, range_counts):\n",
    "        if count > 0:\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                    str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(eval_config.results_dir) / 'performance_dashboard.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "d090e6cba4e3e979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_graph_comparison(before_graph: Data, after_graph: Data,\n",
    "                              episode_info: Dict, save_path: Optional[str] = None) -> None:\n",
    "    \"\"\"Visualize before/after graph comparison with modifications highlighted\"\"\"\n",
    "\n",
    "    # Convert to NetworkX\n",
    "    G_before = to_networkx(before_graph, to_undirected=True)\n",
    "    G_after = to_networkx(after_graph, to_undirected=True)\n",
    "\n",
    "    # Find differences\n",
    "    nodes_before = set(G_before.nodes())\n",
    "    nodes_after = set(G_after.nodes())\n",
    "    edges_before = set(G_before.edges())\n",
    "    edges_after = set(G_after.edges())\n",
    "\n",
    "    added_nodes = nodes_after - nodes_before\n",
    "    removed_nodes = nodes_before - nodes_after\n",
    "    added_edges = edges_after - edges_before\n",
    "    removed_edges = edges_before - edges_after\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # Determine layout (use the same for both graphs)\n",
    "    all_nodes = nodes_before.union(nodes_after)\n",
    "    if len(all_nodes) <= 50:\n",
    "        pos = nx.spring_layout(G_after if len(G_after.nodes()) >= len(G_before.nodes()) else G_before,\n",
    "                              k=1, iterations=50, seed=42)\n",
    "    else:\n",
    "        pos = nx.spring_layout(G_after if len(G_after.nodes()) >= len(G_before.nodes()) else G_before,\n",
    "                              k=3, iterations=30, seed=42)\n",
    "\n",
    "    # Plot BEFORE graph\n",
    "    ax1.set_title(f\"BEFORE Refactoring\\nHub Score: {episode_info['initial_hub_score']:.4f}\",\n",
    "                  fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Draw edges first (so they appear behind nodes)\n",
    "    if G_before.edges():\n",
    "        nx.draw_networkx_edges(G_before, pos, ax=ax1, edge_color=eval_config.colors['original_edges'],\n",
    "                              width=eval_config.edge_width, alpha=0.6)\n",
    "\n",
    "    # Draw nodes\n",
    "    if G_before.nodes():\n",
    "        node_colors = [eval_config.colors['hub_node'] if node == 0 else eval_config.colors['original_nodes']\n",
    "                      for node in G_before.nodes()]\n",
    "        node_sizes = [eval_config.node_size_factor * 1.5 if node == 0 else eval_config.node_size_factor\n",
    "                      for node in G_before.nodes()]\n",
    "\n",
    "        nx.draw_networkx_nodes(G_before, pos, ax=ax1, node_color=node_colors,\n",
    "                              node_size=node_sizes, alpha=0.8)\n",
    "\n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G_before, pos, ax=ax1, font_size=8, font_weight='bold')\n",
    "\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Plot AFTER graph\n",
    "    ax2.set_title(f\"AFTER Refactoring\\nHub Score: {episode_info['final_hub_score']:.4f}\\n\"\n",
    "                 f\"Improvement: {episode_info['hub_improvement']:.4f}\",\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Draw original edges\n",
    "    original_edges = [(u, v) for u, v in G_after.edges() if (u, v) in edges_before or (v, u) in edges_before]\n",
    "    if original_edges:\n",
    "        nx.draw_networkx_edges(G_after, pos, edgelist=original_edges, ax=ax2,\n",
    "                              edge_color=eval_config.colors['original_edges'],\n",
    "                              width=eval_config.edge_width, alpha=0.6)\n",
    "\n",
    "    # Draw added edges\n",
    "    added_edges_list = [(u, v) for u, v in G_after.edges() if (u, v) in added_edges or (v, u) in added_edges]\n",
    "    if added_edges_list:\n",
    "        nx.draw_networkx_edges(G_after, pos, edgelist=added_edges_list, ax=ax2,\n",
    "                              edge_color=eval_config.colors['added_edges'],\n",
    "                              width=eval_config.edge_width * 1.5, alpha=0.8, style='dashed')\n",
    "\n",
    "    # Draw nodes\n",
    "    if G_after.nodes():\n",
    "        node_colors = []\n",
    "        node_sizes = []\n",
    "        for node in G_after.nodes():\n",
    "            if node == 0:  # Hub node\n",
    "                node_colors.append(eval_config.colors['hub_node'])\n",
    "                node_sizes.append(eval_config.node_size_factor * 1.5)\n",
    "            elif node in added_nodes:  # Added nodes\n",
    "                node_colors.append(eval_config.colors['added_nodes'])\n",
    "                node_sizes.append(eval_config.node_size_factor * 1.2)\n",
    "            else:  # Original nodes\n",
    "                node_colors.append(eval_config.colors['original_nodes'])\n",
    "                node_sizes.append(eval_config.node_size_factor)\n",
    "\n",
    "        nx.draw_networkx_nodes(G_after, pos, ax=ax2, node_color=node_colors,\n",
    "                              node_size=node_sizes, alpha=0.8)\n",
    "\n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G_after, pos, ax=ax2, font_size=8, font_weight='bold')\n",
    "\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=eval_config.colors['hub_node'],\n",
    "                  markersize=12, label='Hub Node'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=eval_config.colors['original_nodes'],\n",
    "                  markersize=10, label='Original Nodes'),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=eval_config.colors['added_nodes'],\n",
    "                  markersize=10, label='Added Nodes'),\n",
    "        plt.Line2D([0], [0], color=eval_config.colors['original_edges'], linewidth=2, label='Original Edges'),\n",
    "        plt.Line2D([0], [0], color=eval_config.colors['added_edges'], linewidth=2,\n",
    "                  linestyle='--', label='Added Edges')\n",
    "    ]\n",
    "\n",
    "    fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.02),\n",
    "              ncol=len(legend_elements), fontsize=10)\n",
    "\n",
    "    # Add episode information\n",
    "    info_text = f\"\"\"\n",
    "Episode {episode_info['episode_id']} Summary:\n",
    "• Actions taken: {len(episode_info['actions_taken'])}\n",
    "• Episode reward: {episode_info['episode_reward']:.3f}\n",
    "• Success: {'✅' if episode_info['success'] else '❌'}\n",
    "• Valid actions: {episode_info['num_valid_actions']}/{len(episode_info['actions_taken'])}\n",
    "\"\"\"\n",
    "\n",
    "    plt.figtext(0.02, 0.95, info_text, fontsize=10, fontfamily='monospace',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "\n",
    "    # Add modifications summary\n",
    "    mod_text = f\"\"\"\n",
    "Graph Modifications:\n",
    "• Nodes added: {len(added_nodes)}\n",
    "• Nodes removed: {len(removed_nodes)}\n",
    "• Edges added: {len(added_edges)}\n",
    "• Edges removed: {len(removed_edges)}\n",
    "\"\"\"\n",
    "\n",
    "    plt.figtext(0.98, 0.95, mod_text, fontsize=10, fontfamily='monospace', ha='right',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n"
   ],
   "id": "4cb097bdfd913815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_interactive_trajectory_plot(episode_data: Dict) -> go.Figure:\n",
    "    \"\"\"Create interactive plot of episode trajectory\"\"\"\n",
    "\n",
    "    steps = range(len(episode_data['step_info']) + 1)  # +1 for initial state\n",
    "    hub_scores = [episode_data['initial_hub_score']]\n",
    "    hub_scores.extend([step['hub_score'] for step in episode_data['step_info']])\n",
    "\n",
    "    actions_taken = ['Initial'] + [f\"Action {step['action']}\" for step in episode_data['step_info']]\n",
    "    rewards = [0] + episode_data['rewards']\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Hub Score Trajectory', 'Step Rewards'),\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "\n",
    "    # Hub score trajectory\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=steps,\n",
    "            y=hub_scores,\n",
    "            mode='lines+markers',\n",
    "            name='Hub Score',\n",
    "            line=dict(color='blue', width=3),\n",
    "            marker=dict(size=8),\n",
    "            hovertemplate='<b>Step %{x}</b><br>Hub Score: %{y:.4f}<br>Action: %{text}<extra></extra>',\n",
    "            text=actions_taken\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add success threshold line\n",
    "    fig.add_hline(y=episode_data['initial_hub_score'], row=1, col=1,\n",
    "                  line_dash=\"dash\", line_color=\"red\",\n",
    "                  annotation_text=\"Initial Hub Score\")\n",
    "\n",
    "    # Step rewards\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=steps[1:],  # Exclude initial step\n",
    "            y=rewards[1:],  # Exclude initial reward\n",
    "            name='Step Reward',\n",
    "            marker_color=['green' if r > 0 else 'red' for r in rewards[1:]],\n",
    "            hovertemplate='<b>Step %{x}</b><br>Reward: %{y:.3f}<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Episode {episode_data['episode_id']} Trajectory - \"\n",
    "              f\"Final Improvement: {episode_data['hub_improvement']:.4f}\",\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Step\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Hub Score\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Reward\", row=2, col=1)\n",
    "\n",
    "    return fig"
   ],
   "id": "f89a56c342a9eea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_action_analysis_plot(evaluation_results: Dict) -> go.Figure:\n",
    "    \"\"\"Create interactive action analysis plot\"\"\"\n",
    "\n",
    "    results = evaluation_results['episode_results']\n",
    "    stats = evaluation_results['summary_stats']\n",
    "\n",
    "    action_names = ['RemoveEdge', 'AddEdge', 'MoveEdge', 'ExtractMethod',\n",
    "                   'ExtractAbstractUnit', 'ExtractUnit', 'STOP']\n",
    "\n",
    "    # Prepare data\n",
    "    action_usage = [stats['action_distribution'].get(i, 0) * 100 for i in range(7)]\n",
    "    action_success = [stats['action_success_rates'].get(i, {}).get('rate', 0) * 100 for i in range(7)]\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Action Usage Frequency', 'Action Success Rates'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "\n",
    "    # Action usage\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=action_names,\n",
    "            y=action_usage,\n",
    "            name='Usage %',\n",
    "            marker_color='lightblue',\n",
    "            hovertemplate='<b>%{x}</b><br>Usage: %{y:.1f}%<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Action success rates\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=action_names,\n",
    "            y=action_success,\n",
    "            name='Success %',\n",
    "            marker_color='lightgreen',\n",
    "            hovertemplate='<b>%{x}</b><br>Success Rate: %{y:.1f}%<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Action Analysis Dashboard\",\n",
    "        height=500,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Actions\", row=1, col=1, tickangle=45)\n",
    "    fig.update_xaxes(title_text=\"Actions\", row=1, col=2, tickangle=45)\n",
    "    fig.update_yaxes(title_text=\"Usage Frequency (%)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Success Rate (%)\", row=1, col=2)\n",
    "\n",
    "    return fig"
   ],
   "id": "2d6ccf9335c7ac6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_complete_evaluation():\n",
    "    \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "\n",
    "    print(\"🚀 Starting Complete Evaluation Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create results directory\n",
    "    Path(eval_config.results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load models\n",
    "    print(\"📦 Loading trained models...\")\n",
    "    model, model_checkpoint = load_trained_model(eval_config.model_path, eval_config.device)\n",
    "    discriminator = load_discriminator(eval_config.discriminator_path, eval_config.device)\n",
    "\n",
    "    # Initialize evaluator\n",
    "    print(\"🔄 Initializing evaluator...\")\n",
    "    evaluator = GraphRefactoringEvaluator(model, discriminator, eval_config)\n",
    "\n",
    "    # Run evaluation\n",
    "    print(\"📊 Running comprehensive evaluation...\")\n",
    "    evaluation_results = evaluator.run_comprehensive_evaluation()\n",
    "\n",
    "    # Save results\n",
    "    results_file = Path(eval_config.results_dir) / 'evaluation_results.json'\n",
    "    with open(results_file, 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        serializable_results = {}\n",
    "        for key, value in evaluation_results.items():\n",
    "            if key == 'episode_results':\n",
    "                # Handle episode results specially\n",
    "                serializable_episodes = []\n",
    "                for episode in value:\n",
    "                    episode_copy = episode.copy()\n",
    "                    # Remove non-serializable graph objects\n",
    "                    for graph_key in ['initial_graph', 'final_graph', 'states']:\n",
    "                        if graph_key in episode_copy:\n",
    "                            del episode_copy[graph_key]\n",
    "                    serializable_episodes.append(episode_copy)\n",
    "                serializable_results[key] = serializable_episodes\n",
    "            elif key == 'trajectories':\n",
    "                # Handle trajectories specially (similar to episode_results)\n",
    "                serializable_trajectories = []\n",
    "                for traj in value:\n",
    "                    traj_copy = traj.copy()\n",
    "                    for graph_key in ['initial_graph', 'final_graph', 'states']:\n",
    "                        if graph_key in traj_copy:\n",
    "                            del traj_copy[graph_key]\n",
    "                    serializable_trajectories.append(traj_copy)\n",
    "                serializable_results[key] = serializable_trajectories\n",
    "            else:\n",
    "                serializable_results[key] = value\n",
    "\n",
    "        json.dump(serializable_results, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"💾 Results saved to {results_file}\")\n",
    "\n",
    "    # Create visualizations\n",
    "    print(\"📈 Creating performance dashboard...\")\n",
    "    create_performance_dashboard(evaluation_results)\n",
    "\n",
    "    # Create graph comparisons for trajectory episodes\n",
    "    print(\"🎨 Creating graph visualizations...\")\n",
    "    for i, episode_data in enumerate(evaluation_results['trajectories']):\n",
    "        save_path = Path(eval_config.results_dir) / f'graph_comparison_episode_{i}.png'\n",
    "        visualize_graph_comparison(\n",
    "            episode_data['initial_graph'],\n",
    "            episode_data['final_graph'],\n",
    "            episode_data,\n",
    "            save_path\n",
    "        )\n",
    "\n",
    "    # Create interactive plots\n",
    "    print(\"🎭 Creating interactive visualizations...\")\n",
    "\n",
    "    # Trajectory plots for first few episodes\n",
    "    for i, episode_data in enumerate(evaluation_results['trajectories'][:3]):\n",
    "        trajectory_fig = create_interactive_trajectory_plot(episode_data)\n",
    "        trajectory_fig.write_html(Path(eval_config.results_dir) / f'trajectory_episode_{i}.html')\n",
    "\n",
    "    # Action analysis plot\n",
    "    action_fig = create_action_analysis_plot(evaluation_results)\n",
    "    action_fig.write_html(Path(eval_config.results_dir) / 'action_analysis.html')\n",
    "\n",
    "    print(\"✅ Evaluation completed successfully!\")\n",
    "    print(f\"📁 All results saved to: {eval_config.results_dir}\")\n",
    "\n",
    "    return evaluation_results"
   ],
   "id": "695ed9352ec8cbab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_failure_cases(evaluation_results: Dict) -> None:\n",
    "    \"\"\"Analyze episodes where the model failed to improve\"\"\"\n",
    "\n",
    "    results = evaluation_results['episode_results']\n",
    "    failures = [r for r in results if not r['success']]\n",
    "\n",
    "    if not failures:\n",
    "        print(\"🎉 No failure cases found! All episodes were successful.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n🔍 FAILURE CASE ANALYSIS ({len(failures)} episodes)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Common failure patterns\n",
    "    early_stops = [f for f in failures if f['final_action_was_stop'] and f['episode_length'] <= 3]\n",
    "    long_episodes = [f for f in failures if f['episode_length'] >= 15]\n",
    "    negative_improvements = [f for f in failures if f['hub_improvement'] < -0.01]\n",
    "\n",
    "    print(f\"📊 Failure Patterns:\")\n",
    "    print(f\"   Early stops (≤3 steps): {len(early_stops)} ({len(early_stops)/len(failures):.1%})\")\n",
    "    print(f\"   Long episodes (≥15 steps): {len(long_episodes)} ({len(long_episodes)/len(failures):.1%})\")\n",
    "    print(f\"   Negative improvements: {len(negative_improvements)} ({len(negative_improvements)/len(failures):.1%})\")\n",
    "\n",
    "    # Action patterns in failures\n",
    "    failure_actions = []\n",
    "    for f in failures:\n",
    "        failure_actions.extend(f['actions_taken'])\n",
    "\n",
    "    action_names = ['RemoveEdge', 'AddEdge', 'MoveEdge', 'ExtractMethod',\n",
    "                   'ExtractAbstractUnit', 'ExtractUnit', 'STOP']\n",
    "\n",
    "    print(f\"\\n🎮 Action Usage in Failures:\")\n",
    "    for action in range(7):\n",
    "        usage = failure_actions.count(action) / len(failure_actions) if failure_actions else 0\n",
    "        print(f\"   {action_names[action]}: {usage:.1%}\")\n",
    "\n",
    "    # Average metrics for failures\n",
    "    avg_failure_reward = np.mean([f['episode_reward'] for f in failures])\n",
    "    avg_failure_length = np.mean([f['episode_length'] for f in failures])\n",
    "    avg_failure_improvement = np.mean([f['hub_improvement'] for f in failures])\n",
    "\n",
    "    print(f\"\\n📉 Failure Metrics:\")\n",
    "    print(f\"   Average reward: {avg_failure_reward:.3f}\")\n",
    "    print(f\"   Average length: {avg_failure_length:.1f}\")\n",
    "    print(f\"   Average improvement: {avg_failure_improvement:.4f}\")\n",
    "\n",
    "def analyze_success_patterns(evaluation_results: Dict) -> None:\n",
    "    \"\"\"Analyze patterns in successful episodes\"\"\"\n",
    "\n",
    "    results = evaluation_results['episode_results']\n",
    "    successes = [r for r in results if r['success']]\n",
    "\n",
    "    if not successes:\n",
    "        print(\"❌ No successful episodes found!\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n🎯 SUCCESS PATTERN ANALYSIS ({len(successes)} episodes)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Success tiers\n",
    "    excellent = [s for s in successes if s['hub_improvement'] > 0.1]\n",
    "    good = [s for s in successes if 0.05 < s['hub_improvement'] <= 0.1]\n",
    "    moderate = [s for s in successes if 0.01 < s['hub_improvement'] <= 0.05]\n",
    "\n",
    "    print(f\"📈 Success Tiers:\")\n",
    "    print(f\"   Excellent (>0.1): {len(excellent)} ({len(excellent)/len(successes):.1%})\")\n",
    "    print(f\"   Good (0.05-0.1): {len(good)} ({len(good)/len(successes):.1%})\")\n",
    "    print(f\"   Moderate (0.01-0.05): {len(moderate)} ({len(moderate)/len(successes):.1%})\")\n",
    "\n",
    "    # Optimal episode lengths\n",
    "    success_lengths = [s['episode_length'] for s in successes]\n",
    "    print(f\"\\n📏 Episode Length Analysis:\")\n",
    "    print(f\"   Mean length: {np.mean(success_lengths):.1f}\")\n",
    "    print(f\"   Median length: {np.median(success_lengths):.1f}\")\n",
    "    print(f\"   Range: {min(success_lengths)}-{max(success_lengths)}\")\n",
    "\n",
    "    # Most effective actions in successful episodes\n",
    "    success_actions = []\n",
    "    for s in successes:\n",
    "        success_actions.extend(s['actions_taken'])\n",
    "\n",
    "    action_names = ['RemoveEdge', 'AddEdge', 'MoveEdge', 'ExtractMethod',\n",
    "                   'ExtractAbstractUnit', 'ExtractUnit', 'STOP']\n",
    "\n",
    "    print(f\"\\n🎮 Action Usage in Successes:\")\n",
    "    for action in range(7):\n",
    "        usage = success_actions.count(action) / len(success_actions) if success_actions else 0\n",
    "        print(f\"   {action_names[action]}: {usage:.1%}\")\n",
    "\n",
    "def create_comparative_analysis(evaluation_results: Dict) -> None:\n",
    "    \"\"\"Create comparative analysis between successful and failed episodes\"\"\"\n",
    "\n",
    "    results = evaluation_results['episode_results']\n",
    "    successes = [r for r in results if r['success']]\n",
    "    failures = [r for r in results if not r['success']]\n",
    "\n",
    "    print(f\"\\n⚖️  COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not successes or not failures:\n",
    "        print(\"Cannot perform comparative analysis - need both successes and failures\")\n",
    "        return\n",
    "\n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = {\n",
    "        'Metric': [\n",
    "            'Average Episode Length',\n",
    "            'Average Episode Reward',\n",
    "            'Average Valid Actions',\n",
    "            'Average Hub Score Initial',\n",
    "            'Early Stop Rate (%)',\n",
    "            'Action RemoveEdge (%)',\n",
    "            'Action AddEdge (%)',\n",
    "            'Action ExtractMethod (%)',\n",
    "            'Action STOP (%)'\n",
    "        ],\n",
    "        'Successful Episodes': [\n",
    "            f\"{np.mean([s['episode_length'] for s in successes]):.1f}\",\n",
    "            f\"{np.mean([s['episode_reward'] for s in successes]):.3f}\",\n",
    "            f\"{np.mean([s['num_valid_actions'] for s in successes]):.1f}\",\n",
    "            f\"{np.mean([s['initial_hub_score'] for s in successes]):.4f}\",\n",
    "            f\"{sum(1 for s in successes if s['final_action_was_stop'] and s['episode_length'] <= 3) / len(successes) * 100:.1f}\",\n",
    "            f\"{sum(s['actions_taken'].count(0) for s in successes) / sum(len(s['actions_taken']) for s in successes) * 100:.1f}\",\n",
    "            f\"{sum(s['actions_taken'].count(1) for s in successes) / sum(len(s['actions_taken']) for s in successes) * 100:.1f}\",\n",
    "            f\"{sum(s['actions_taken'].count(3) for s in successes) / sum(len(s['actions_taken']) for s in successes) * 100:.1f}\",\n",
    "            f\"{sum(s['actions_taken'].count(6) for s in successes) / sum(len(s['actions_taken']) for s in successes) * 100:.1f}\",\n",
    "        ],\n",
    "        'Failed Episodes': [\n",
    "            f\"{np.mean([f['episode_length'] for f in failures]):.1f}\",\n",
    "            f\"{np.mean([f['episode_reward'] for f in failures]):.3f}\",\n",
    "            f\"{np.mean([f['num_valid_actions'] for f in failures]):.1f}\",\n",
    "            f\"{np.mean([f['initial_hub_score'] for f in failures]):.4f}\",\n",
    "            f\"{sum(1 for f in failures if f['final_action_was_stop'] and f['episode_length'] <= 3) / len(failures) * 100:.1f}\",\n",
    "            f\"{sum(f['actions_taken'].count(0) for f in failures) / sum(len(f['actions_taken']) for f in failures) * 100:.1f}\",\n",
    "            f\"{sum(f['actions_taken'].count(1) for f in failures) / sum(len(f['actions_taken']) for f in failures) * 100:.1f}\",\n",
    "            f\"{sum(f['actions_taken'].count(3) for f in failures) / sum(len(f['actions_taken']) for f in failures) * 100:.1f}\",\n",
    "            f\"{sum(f['actions_taken'].count(6) for f in failures) / sum(len(f['actions_taken']) for f in failures) * 100:.1f}\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(df.to_string(index=False))"
   ],
   "id": "cf7e0b660541cd18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if model exists\n",
    "    if not Path(eval_config.model_path).exists():\n",
    "        print(f\"❌ Model not found at {eval_config.model_path}\")\n",
    "        print(\"Please train the model first or update the model path.\")\n",
    "    else:\n",
    "        # Run complete evaluation\n",
    "        results = run_complete_evaluation()\n",
    "\n",
    "        # Additional analyses\n",
    "        analyze_failure_cases(results)\n",
    "        analyze_success_patterns(results)\n",
    "        create_comparative_analysis(results)\n",
    "\n",
    "        print(f\"\\n🎉 Complete evaluation finished!\")\n",
    "        print(f\"📊 Check {eval_config.results_dir} for all visualizations and results\")\n",
    "\n",
    "# =============================================================================\n",
    "# NOTEBOOK EXECUTION CELLS\n",
    "# =============================================================================\n",
    "\n",
    "# Cell 1: Setup and Configuration\n",
    "print(\"📝 Notebook ready! Run the cells below to execute the evaluation:\")\n",
    "print(\"\\n1. First, ensure your model paths are correct in EvaluationConfig\")\n",
    "print(\"2. Run run_complete_evaluation() to start the evaluation\")\n",
    "print(\"3. Check the results directory for all generated visualizations\")\n",
    "\n",
    "# Cell 2: Quick Model Check\n",
    "def quick_model_check():\n",
    "    \"\"\"Quick check if models are available\"\"\"\n",
    "    model_exists = Path(eval_config.model_path).exists()\n",
    "    disc_exists = Path(eval_config.discriminator_path).exists()\n",
    "    data_exists = Path(eval_config.data_path).exists()\n",
    "\n",
    "    print(\"🔍 MODEL AVAILABILITY CHECK:\")\n",
    "    print(f\"   Main model: {'✅' if model_exists else '❌'} {eval_config.model_path}\")\n",
    "    print(f\"   Discriminator: {'✅' if disc_exists else '❌'} {eval_config.discriminator_path}\")\n",
    "    print(f\"   Data: {'✅' if data_exists else '❌'} {eval_config.data_path}\")\n",
    "\n",
    "    return model_exists and data_exists"
   ],
   "id": "3a4fc681f8bf613e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
